<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Guide: Prompt Engineering for Gemini</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <!-- Chosen Palette: Cool Neutrals -->
    <!-- Application Structure Plan: The application is designed as a single-page experience with a fixed top navigation, allowing users to jump between key themes of the course outline. This non-linear, thematic structure (Introduction, Foundations, Advanced, Meta Prompting, Practice, Labs) is user-friendly for learning, allowing exploration based on interest. Each section uses interactive cards, toggles, and dynamic visualizations to break down complex information into digestible, engaging pieces, promoting active learning and exploration. This dashboard-like approach transforms the dense course material into an accessible and synthesizable guide. Authentication ensures restricted access. -->
    <!-- Visualization & Content Choices: 
        - Course outline text -> Integrated directly into sections for comprehensive content.
        - Table 1 (Core Prompting Techniques) - Interactive Cards & Modal: Each technique is a clickable card revealing detailed description, use cases, common mistakes, and Gemini-specific considerations in a modal. Includes LLM-powered example generation. (Goal: Inform, Interact)
        - Table 2 (Model Parameters) -> Interactive Sliders: Parameters like Temperature, Top-K, Top-P are controlled by sliders, with dynamic descriptions of their impact. (Goal: Explore Relationships, Interact)
        - CoT/ToT/GoT Progression -> HTML/CSS Flow Diagram: The evolution of reasoning techniques is shown as a visual flow for clarity. (Goal: Show Change, Inform)
        - Gemini's 1M Token Context -> Chart.js Bar Chart: The scale of the context window isÂèØËßÜÂåñ to make an abstract number concrete and impactful. (Goal: Compare, Inform)
        - Multimodality -> Icon-based Cards: Gemini's multimodal features are represented with clear icons and short descriptions for quick overview. (Goal: Inform)
        - Table 3 (Hands-on Labs) -> Interactive Expandable List: Labs are presented as clickable, expandable items, revealing objectives and example tasks. Includes LLM-powered hint generation. (Goal: Organize, Interact)
        - Justification for all choices is to transform static course outline text into interactive experiences that enhance comprehension and retention, which is the primary goal of an educational application.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #F8F9FA;
            color: #212529;
            -webkit-text-size-adjust: 100%;
            text-size-adjust: 100%;
        }
        .nav-link {
            transition: all 0.3s ease;
            cursor: pointer;
            border-bottom: 2px solid transparent;
        }
        .nav-link.active, .nav-link:hover {
            color: #3B82F6;
            border-bottom-color: #3B82F6;
        }
        .content-section {
            display: none;
        }
        .content-section.active {
            display: block;
        }
        .card {
            background-color: white;
            border-radius: 0.75rem;
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -2px rgb(0 0 0 / 0.1);
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
            height: 800px;
            max-height: 50vh;
        }
        .lab-item .lab-details {
            max-height: 200px;
            overflow: hidden;
            transition: max-height 0.5s ease-out;
        }
        .lab-item.open .lab-details {
            max-height: 2000px; /* Adjusted for new content */
            transition: max-height 0.5s ease-in;
        }
        .llm-output {
            background-color: #e0f2fe;
            border-left: 4px solid #3b82f6;
            padding: 1rem;
            margin-top: 1rem;
            border-radius: 0.5rem;
            font-family: monospace;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        /* Added for pre tags within modal and lab details */
        .llm-output pre,
        .modal-body .bg-gray-100 pre,
        .lab-details .bg-gray-100 pre {
            max-width: 100%; /* Ensure pre content doesn't overflow */
            overflow-x: auto; /* Add horizontal scroll if content is too wide */
        }

        .loading-text {
            color: #6b7280;
            font-style: italic;
            margin-top: 0.5rem;
        }
        /* Explicit styling for the mobile menu button to ensure visibility */
        @media (max-width: 767px) { /* Tailwind's 'md' breakpoint is typically 768px */
            #mobile-menu-button {
                display: block !important; /* Ensure it's displayed */
                padding: 0.5rem; /* Add padding */
                border: 1px solid #ccc; /* Add a border */
                border-radius: 0.25rem; /* Rounded corners */
                background-color: #f0f0f0; /* Light background */
            }
        }
    </style>
</head>
<body class="antialiased">

    <!-- Login Screen -->
    <div id="login-screen" class="flex items-center justify-center min-h-screen bg-gray-100">
        <div class="bg-white p-8 rounded-lg shadow-md w-full max-w-sm text-center">
            <h2 class="text-2xl font-bold mb-6">Login to Access</h2>
            <div class="mb-4">
                <input id="login-email" type="email" placeholder="Email" class="w-full p-2 mb-2 border rounded" autocomplete="username">
                <input id="login-password" type="password" placeholder="Password" class="w-full p-2 mb-2 border rounded" autocomplete="current-password">
                <button onclick="loginWithEmailPassword()" class="w-full bg-blue-600 text-white p-3 rounded-md hover:bg-blue-700 transition-colors mt-2">Login with Email</button>
            </div>
            <div class="mb-2 text-gray-500">or</div>
            <button onclick="loginWithGoogle()" class="w-full bg-red-600 text-white p-3 rounded-md hover:bg-red-700 transition-colors mt-2 flex items-center justify-center">
                <svg class="w-5 h-5 mr-2" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M12.0003 4.75C14.0263 4.75 15.8263 5.485 17.2003 6.81L19.9703 4.04C17.9603 2.075 15.1703 1 12.0003 1C7.57031 1 3.73031 3.425 1.83031 7.02L5.50031 9.87C6.48031 7.855 9.03531 6.5 12.0003 6.5C13.5603 6.5 14.9303 7.03 16.0003 7.91L16.0003 7.92L16.0003 7.91C16.5803 8.395 17.0653 8.945 17.4403 9.55L17.4503 9.555L17.4403 9.55C17.7603 10.055 18.0103 10.61 18.1903 11.2L18.2003 11.2L18.1903 11.2C18.2603 11.435 18.3103 11.69 18.3303 11.95L18.3303 11.965L18.3303 11.95C18.3303 12.08 18.3303 12.21 18.3303 12.34L18.3303 12.35L18.3303 12.34C18.3303 12.47 18.3303 12.6 18.3303 12.73L18.3303 12.745L18.3303 12.73C18.3303 12.86 18.3303 12.99 18.3303 13.12L18.3303 13.135L18.3303 13.12C18.3303 13.25 18.3303 13.38 18.3303 13.51L18.3303 13.525L18.3303 13.51C18.3303 13.64 18.3303 13.77 18.3303 13.9L18.3303 13.915L18.3303 13.9C18.3303 14.03 18.3303 14.16 18.3303 14.29L18.3303 14.305L18.3303 14.29C18.3303 14.42 18.3303 14.55 18.3303 14.68L18.3303 14.695L18.3303 14.68C18.3303 14.81 18.3303 14.94 18.3303 15.07L18.3303 15.085L18.3303 15.07C18.3303 15.2 18.3303 15.33 18.3303 15.46L18.3303 15.475L18.3303 15.46C18.3303 15.59 18.3303 15.72 18.3303 15.85L18.3303 15.865L18.3303 15.85C18.3303 15.98 18.3303 16.11 18.3303 16.24L18.3303 16.255L18.3303 16.24C18.3303 16.37 18.3303 16.5 18.3303 16.63L18.3303 16.645L18.3303 16.63C18.3303 16.76 18.3303 16.89 18.3303 17.02L18.3303 17.035L18.3303 17.02C18.3303 17.15 18.3303 17.28 18.3303 17.41L18.3303 17.425L18.3303 17.41C18.3303 17.54 18.3303 17.67 18.3303 17.8L18.3303 17.815L18.3303 17.8C18.3303 17.93 18.3303 18.06 18.3303 18.19L18.3303 18.205L18.3303 18.19C18.3303 18.32 18.3303 18.45 18.3303 18.58L18.3303 18.595L18.3303 18.58C18.3303 18.71 18.3303 18.84 18.3303 18.97L18.3303 18.985L18.3303 18.97C18.3303 19.1 18.3303 19.23 18.3303 19.36L18.3303 19.375L18.3303 19.36C18.3303 19.49 18.3303 19.62 18.3303 19.75L18.3303 19.765L18.3303 19.75C18.3303 19.88 18.3303 20.01 18.3303 20.14L18.3303 20.155L18.3303 20.14C18.3303 20.27 18.3303 20.4 18.3303 20.53L18.3303 20.545L18.3303 20.53C18.3303 20.66 18.3303 20.79 18.3303 20.92L18.3303 20.935L18.3303 20.92C18.3303 21.05 18.3303 21.18 18.3303 21.31L18.3303 21.325L18.3303 21.31C18.3303 21.44 18.3303 21.57 18.3303 21.7L18.3303 21.715L18.3303 21.7C18.3303 21.83 18.3303 21.96 18.3303 22.09L18.3303 22.105L18.3303 22.09C18.3303 22.22 18.3303 22.35 18.3303 22.48L18.3303 22.495L18.3303 22.48C18.3303 22.61 18.3303 22.74 18.3303 22.87L18.3303 22.885L18.3303 22.87C18.3303 23 18.3303 23 12.0003 23C6.48031 23 2.00031 18.52 2.00031 13C2.00031 8.845 4.30031 5.255 7.78031 3.44L9.00031 5.465C7.05031 6.55 5.75031 8.525 5.75031 10.75C5.75031 13.945 8.30531 16.5 11.5003 16.5C14.6953 16.5 17.2503 13.945 17.2503 10.75C17.2503 9.475 16.7803 8.305 16.0003 7.37L16.0003 7.38L16.0003 7.37C15.2203 6.435 14.0503 5.75 12.7753 5.75L12.7703 5.75H12.7753C12.4853 5.75 12.2303 5.75 12.0003 5.75C10.7253 5.75 9.55531 6.22 8.62031 7L7.62031 6.005L7.62031 6.005L7.62031 6.005C8.55531 5.07 9.72531 4.5 11.0003 4.5C11.3903 4.5 11.7703 4.545 12.0003 4.545L12.0003 4.75Z" clip-rule="evenodd" fill-rule="evenodd"></path>
                </svg>
                Sign in with Google
            </button>
            <p id="login-error-message" class="text-red-500 text-sm mt-4"></p>
        </div>
    </div>

    <!-- Main Application Content -->
    <div id="app-content" class="hidden">
        <header class="bg-white/80 backdrop-blur-md sticky top-0 z-50 shadow-sm">
            <nav class="container mx-auto px-4 sm:px-6 lg:px-8">
                <div class="flex items-center justify-between h-16">
                    <div class="flex items-center">
                        <span class="font-bold text-xl text-gray-800 cursor-pointer" onclick="showSection('introduction')" title="Go to Introduction">Gemini Prompting Guide</span>
                    </div>
                    <!-- Hamburger icon for mobile -->
                    <div class="md:hidden">
                        <button id="mobile-menu-button" class="text-gray-700 hover:text-blue-500 focus:outline-none focus:text-blue-500" title="Open navigation menu">
                            <svg class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />
                            </svg>
                        </button>
                    </div>
                    <!-- Desktop navigation -->
                    <div class="hidden md:block">
                        <div class="ml-10 flex items-baseline space-x-4">
                            <a onclick="showSection('introduction')" class="nav-link active px-3 py-2 rounded-md text-sm font-medium text-gray-700">Introduction</a>
                            <a onclick="showSection('foundations')" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-gray-700">Foundations</a>
                            <a onclick="showSection('advanced')" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-gray-700">Advanced</a>
                            <a onclick="showSection('meta-prompting')" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-gray-700">Meta Prompting</a>
                            <a onclick="showSection('practice')" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-gray-700">In Practice</a>
                            <a onclick="showSection('labs')" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-gray-700">Labs</a>
                            <a onclick="showSection('insomnia-cure')" class="nav-link px-3 py-2 rounded-md text-sm font-medium text-gray-700">Insomnia Cure</a>
                        </div>
                    </div>
                    <button onclick="logoutUser()" class="px-3 py-2 bg-red-500 text-white rounded-md hover:bg-red-600 transition-colors text-sm font-medium">Logout</button>
                </div>
                <!-- Mobile menu content -->
                <div id="mobile-menu" class="md:hidden hidden">
                    <div class="px-2 pt-2 pb-3 space-y-1 sm:px-3">
                        <a onclick="showSection('introduction'); toggleMobileMenu();" class="block px-3 py-2 rounded-md text-base font-medium text-gray-700 hover:bg-gray-100">Introduction</a>
                        <a onclick="showSection('foundations'); toggleMobileMenu();" class="block px-3 py-2 rounded-md text-base font-medium text-gray-700 hover:bg-gray-100">Foundations</a>
                        <a onclick="showSection('advanced'); toggleMobileMenu();" class="block px-3 py-2 rounded-md text-base font-medium text-gray-700 hover:bg-gray-100">Advanced</a>
                        <a onclick="showSection('meta-prompting'); toggleMobileMenu();" class="block px-3 py-2 rounded-md text-base font-medium text-gray-700 hover:bg-gray-100">Meta Prompting</a>
                        <a onclick="showSection('practice'); toggleMobileMenu();" class="block px-3 py-2 rounded-md text-base font-medium text-gray-700 hover:bg-gray-100">In Practice</a>
                        <a onclick="showSection('labs'); toggleMobileMenu();" class="block px-3 py-2 rounded-md text-base font-medium text-gray-700 hover:bg-gray-100">Labs</a>
                        <a onclick="showSection('insomnia-cure'); toggleMobileMenu();" class="block px-3 py-2 rounded-md text-base font-medium text-gray-700 hover:bg-gray-100">Insomnia Cure</a>
                    </div>
                </div>
            </nav>
        </header>

        <main class="container mx-auto px-4 sm:px-6 lg:px-8 py-8">
            <!-- Introduction Section -->
            <section id="introduction" class="content-section active">
                <div class="text-center mb-12">
                    <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 mb-4">Mastering Prompt Engineering for Gemini</h1>
                    <p class="text-lg text-gray-600 max-w-3xl mx-auto">This guide translates the comprehensive 'Prompt Engineering for Gemini' course outline into an interactive experience. Explore foundational concepts, advanced techniques, and practical applications to unlock the full potential of Google's Gemini models.</p>
                </div>
                <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-8">
                    <div class="card p-6">
                        <h3 class="text-xl font-bold mb-2">What is Prompt Engineering?</h3>
                        <p class="text-gray-600">The systematic process of designing and optimizing input prompts to guide Large Language Models (LLMs), enhancing accuracy, relevance, coherence, and overall usability. This methodical approach aims to ensure that the generated output exhibits high levels of accuracy, relevance, coherence, and overall usability. A central objective of this discipline is to enhance the functional effectiveness of LLMs without requiring any modification to their underlying core model parameters.</p>
                    </div>
                    <div class="card p-6">
                        <h3 class="text-xl font-bold mb-2">Historical Context and Evolution</h3>
                        <p class="text-gray-600">The domain of prompt engineering has undergone a significant evolution, tracing its origins back to early rule-based inputs in the 1950s. Its progression continued through key milestones in machine learning and deep learning, experiencing a notable acceleration post-2017 with the advent of transformer architectures and models such as GPT. What began largely as an empirical practice, relying on trial-and-error, has since matured into a well-structured research domain, extending its influence across numerous disciplines.</p>
                    </div>
                    <div class="card p-6 md:col-span-2 lg:col-span-1">
                        <h3 class="text-xl font-bold mb-2">Why Prompt Engineering is Crucial for Gemini</h3>
                        <p class="text-gray-600">Prompt engineering holds particular importance for Google's Gemini models due to their inherent design. Gemini models are engineered with massive context capabilities, enabling powerful in-context learning. This architectural strength necessitates effective prompt engineering to fully exploit features like its expansive 1 million token context window and native multimodal understanding. Achieving precise and desirable output is essential, especially as generative AI models transition from experimental novelties to integral tools embedded within real-world products and services.</p>
                    </div>
                </div>
            </section>

            <!-- Foundations Section -->
            <section id="foundations" class="content-section">
                <div class="text-center mb-12">
                    <h2 class="text-3xl font-bold text-gray-900 mb-2">Foundational Concepts</h2>
                    <p class="text-lg text-gray-600">The building blocks of effective prompting. Click on a technique to learn more about its definition, use cases, common mistakes, and Gemini-specific considerations.</p>
                </div>
                <div id="techniques-container" class="grid md:grid-cols-2 lg:grid-cols-4 gap-6 mb-12"></div>
                
                <div class="bg-white p-8 rounded-xl shadow-md">
                    <h3 class="text-2xl font-bold text-center mb-6">Interactive Parameter Explorer</h3>
                    <p class="text-gray-600 text-center mb-6">Experiment with Gemini's key inference parameters to understand how they influence output randomness, focus, and diversity.</p>
                    <div class="grid md:grid-cols-3 gap-8">
                        <div id="param-temp">
                            <label for="temperature" class="font-semibold">Temperature: <span id="temp-value">0.7</span></label>
                            <input type="range" id="temperature" min="0" max="2" step="0.1" value="0.7" class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer">
                            <p id="temp-desc" class="text-sm text-gray-600 mt-2">Controls the randomness of the output. Higher values (e.g., 1.0-2.0) lead to more creative and diverse responses, while lower values (e.g., 0.0-0.5) result in more deterministic and focused output.</p>
                        </div>
                        <div id="param-topk">
                            <label for="top-k" class="font-semibold">Top-K: <span id="topk-value">20</span></label>
                            <input type="range" id="top-k" min="1" max="40" step="1" value="20" class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer">
                            <p id="topk-desc" class="text-sm text-gray-600 mt-2">Determines the number of top-k most likely next tokens from which the model samples. A lower Top-K value means the model considers fewer options, leading to more focused and less surprising output.</p>
                        </div>
                        <div id="param-topp">
                            <label for="top-p" class="font-semibold">Top-P: <span id="topp-value">0.9</span></label>
                            <input type="range" id="top-p" min="0" max="1" step="0.05" value="0.9" class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer">
                            <p id="topp-desc" class="text-sm text-gray-600 mt-2">Selects from the smallest possible set of tokens whose cumulative probability exceeds the value 'p'. This provides dynamic control over vocabulary, allowing for both broad and narrow sampling depending on the context.</p>
                        </div>
                    </div>
                </div>
            </section>
            
            <!-- Advanced Section -->
            <section id="advanced" class="content-section">
                <div class="text-center mb-12">
                    <h2 class="text-3xl font-bold text-gray-900 mb-2">Advanced Prompting Techniques</h2>
                    <p class="text-lg text-gray-600">Evolving from simple instructions to complex reasoning frameworks, these techniques unlock deeper capabilities.</p>
                </div>

                <div class="space-y-10" id="advanced-techniques-container">
                    <!-- Advanced techniques will be rendered here by JavaScript -->
                </div>
            </section>

            <!-- Meta Prompting Section -->
            <section id="meta-prompting" class="content-section">
                <div class="text-center mb-12">
                    <h2 class="text-3xl font-bold text-gray-900 mb-2">Meta Prompting: Engineering the Prompt Engineer</h2>
                    <p class="text-lg text-gray-600">Meta-prompting involves instructing the LLM itself to generate, optimize, or analyze prompts. It's about using the model to assist in the prompt engineering process, making it more efficient and scalable.</p>
                </div>

                <div class="space-y-10">
                    <div class="card p-6">
                        <h3 class="text-xl font-bold text-blue-600 mb-3">Prompt Generation</h3>
                        <p class="text-gray-600 mb-4">Instructing the LLM to create new prompts based on a given task or desired output.</p>
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 1 (Zero-shot): Generate a prompt for a simple fact-based question.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Generate a zero-shot prompt that asks an AI to state the capital of France."</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 2 (Few-shot): Generate a prompt for sentiment analysis.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Generate a few-shot prompt to classify text sentiment as positive, negative, or neutral, given these examples:\n\n'This movie was great!' -> Positive\n'I hated the ending.' -> Negative\n'It was okay.' -> Neutral\n\nNow, generate the prompt for a new review."</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 3 (Instruction-based): Generate a prompt for summarizing with constraints.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Generate an instruction-based prompt that summarizes a given paragraph in exactly two sentences, maintaining its original meaning."</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 4 (Role-based): Generate a prompt for a historical expert persona.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Generate a role-based prompt where the AI acts as a 19th-century historian explaining the causes of the Industrial Revolution."</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 5 (Chain-of-Thought): Generate a prompt for multi-step math problem.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Generate a Chain-of-Thought prompt to solve the following problem: 'If a baker makes 100 cookies, sells half, and then gives 15 to charity, how many cookies are left? Think step by step.'"</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 6 (Tree-of-Thought): Generate a prompt for complex decision-making.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Generate a Tree-of-Thought prompt for an AI to plan a weekend trip for a family with diverse interests (e.g., adventure, relaxation, culture), exploring multiple itinerary options and evaluating pros and cons for each."</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 7 (Graph-of-Thoughts): Generate a prompt for a research project.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Generate a Graph-of-Thoughts prompt for an AI to conduct a mini-research project on renewable energy sources, breaking it down into interconnected nodes like 'Solar Power Pros/Cons', 'Wind Power Pros/Cons', 'Geothermal Overview', and 'Comparative Analysis', allowing for feedback loops between nodes."</pre>
                            </div>
                        </div>
                    </div>

                    <div class="card p-6">
                        <h3 class="text-xl font-bold text-blue-600 mb-3">Prompt Optimization/Refinement</h3>
                        <p class="text-gray-600 mb-4">Using the LLM to improve existing prompts for better performance, clarity, or adherence to specific constraints.</p>
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 1: Refine a vague prompt.</p>
                                <pre class="whitespace-pre-wrap text-sm">"The following prompt is too vague: 'Write about cars.' Optimize it to produce a short, engaging description of electric vehicles, highlighting their environmental benefits and acceleration."</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 2: Make a prompt more concise.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Refine this prompt for conciseness: 'I want you to act as a travel guide and tell me about the best places to visit in Paris for someone who has never been there before, and also include some hidden gems and local tips.' "</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 3: Add constraints to a prompt.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Improve this prompt by adding a word count limit and a specific tone: 'Describe the history of artificial intelligence.' "</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 4: Optimize for a specific output format.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Take the following prompt and modify it to ensure the output is always in a JSON format with 'title' and 'summary' keys: 'Summarize the given news article.'"</pre>
                            </div>
                        </div>
                    </div>

                    <div class="card p-6">
                        <h3 class="text-xl font-bold text-blue-600 mb-3">Prompt Analysis/Evaluation</h3>
                        <p class="text-gray-600 mb-4">Asking the LLM to analyze a prompt's effectiveness, identify potential issues, or suggest improvements based on a given context or expected output.</p>
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 1: Analyze prompt clarity.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Analyze the following prompt for clarity and potential ambiguities: 'Explain quantum physics simply.' Suggest improvements."</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 2: Evaluate prompt for bias.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Review this prompt for potential biases and suggest how to make it more neutral: 'Write a story about a successful CEO.'"</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 3: Predict output quality.</p>
                                <pre class="whitespace-pre-wrap text-sm">"Given the prompt 'List 10 benefits of exercise', how likely is an LLM to generate a comprehensive and accurate list? What are potential shortcomings?"</pre>
                            </div>
                            <div class="bg-gray-100 p-4 rounded-md">
                                <p class="font-semibold">Example 4: Suggest alternative phrasing.</p>
                                <pre class="whitespace-pre-wrap text-sm">"The prompt 'Tell me about the weather' is too broad. Suggest 3 alternative phrasings for getting specific weather information (e.g., current temperature, 5-day forecast)."</pre>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- In Practice Section -->
            <section id="practice" class="content-section">
                <div class="text-center mb-12">
                    <h2 class="text-3xl font-bold text-gray-900 mb-2">Gemini in Practice</h2>
                    <p class="text-lg text-gray-600">Leveraging Gemini's unique, state-of-the-art features for real-world applications.</p>
                </div>
                <div class="grid lg:grid-cols-5 gap-8">
                    <div class="lg:col-span-3 card p-6">
                        <h3 class="text-xl font-bold mb-4">Unlocking the 1M Token Context Window</h3>
                        <p class="text-gray-600 mb-6">Gemini distinguishes itself as the first generative model capable of processing an unprecedented 1 million tokens in a single context window. This represents a significant leap from earlier models, which were typically limited to 8,000, 32,000, or 128,000 tokens. To provide a tangible scale, a 1 million token context window is equivalent to approximately 50,000 lines of code (assuming 80 characters per line), the entirety of text messages sent over five years, eight average-length English novels, or the transcripts of over 200 average-length podcast episodes.</p>
                        <div class="chart-container">
                            <canvas id="contextChart"></canvas>
                        </div>
                        <p class="text-gray-600 text-sm mt-4">
                            This vast context eliminates the need for complex workarounds like Retrieval-Augmented Generation (RAG) for many tasks, allowing for deeper understanding and synthesis of large amounts of information. It enables tasks such as summarizing entire books, analyzing long legal documents, or conducting in-depth Q&A over extensive datasets without losing context.
                        </p>
                    </div>
                    <div class="lg:col-span-2 card p-6">
                        <h3 class="text-xl font-bold mb-4">Native Multimodality</h3>
                        <p class="text-gray-600 mb-6">Gemini models are architected to be inherently multimodal from their foundation, offering native understanding of text, video, audio, and images.[3, 20] This design unlocks a broad spectrum of image processing and computer vision tasks, such as image captioning, classification, and visual question answering, without the need for training specialized machine learning models.[20] For larger media files, the File API supports uploads of up to 2GB per file, with a project limit of 20GB, enabling their seamless inclusion in prompts.</p>
                        <div class="space-y-4">
                            <div class="flex items-start space-x-4 p-4 bg-gray-50 rounded-lg">
                                <span class="text-2xl">üñºÔ∏è</span>
                                <div>
                                    <h4 class="font-semibold">Image & Visual Reasoning</h4>
                                    <p class="text-sm text-gray-600">Facilitates image captioning, classification, and Visual Question Answering (VQA) across diverse visual content, including charts, natural photographs, and memes.</p>
                                </div>
                            </div>
                            <div class="flex items-start space-x-4 p-4 bg-gray-50 rounded-lg">
                                <span class="text-2xl">üé¨</span>
                                <div>
                                    <h4 class="font-semibold">Video Understanding</h4>
                                    <p class="text-sm text-gray-600">Extends capabilities to video Q&A, video memory (e.g., Project Astra), generating captions, and supporting real-time video processing applications.</p>
                                </div>
                            </div>
                            <div class="flex items-start space-x-4 p-4 bg-gray-50 rounded-lg">
                                <span class="text-2xl">üîä</span>
                                <div>
                                    <h4 class="font-semibold">Audio & Speech Processing</h4>
                                    <p class="text-sm text-gray-600">Supports real-time transcription, translation, Q&A based on audio, and summarization of meeting transcripts.</p>
                                </div>
                            </div>
                            <div class="flex items-start space-x-4 p-4 bg-gray-50 rounded-lg">
                                <span class="text-2xl">üìù</span>
                                <div>
                                    <h4 class="font-semibold">Interleaved Modalities</h4>
                                    <p class="text-sm text-gray-600">Gemini Flash can generate interleaved images alongside text responses, creating rich, integrated content like illustrated recipes or stories within a single interaction.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
            
            <!-- Labs Section -->
            <section id="labs" class="content-section">
                <div class="text-center mb-12">
                    <h2 class="text-3xl font-bold text-gray-900 mb-2">Hands-On Learning Path</h2>
                    <p class="text-lg text-gray-900">Apply your knowledge with these practical lab exercises to solidify your understanding of prompt engineering for Gemini. Click to expand each lab for details and a hint.</p>
                </div>
                <div id="labs-container" class="max-w-4xl mx-auto space-y-4"></div>
            </section>

            <!-- Insomnia Cure Section -->
            <section id="insomnia-cure" class="content-section">
                <div class="text-center mb-12">
                    <!-- <h2 class="text-3xl font-bold text-gray-900 mb-2">Course Outline: Prompt Engineering for Gemini</h2> -->
                    <!-- <p class="text-lg text-gray-600">Content to help you with Insomnia.</p> -->
                </div>
                <div class="bg-white p-8 rounded-xl shadow-md">
                    <!-- Content from PromptEngineering.html will be loaded here -->
                    <div id="insomnia-cure-content" class="text-gray-700">
                        <html>
                          <head>
                            <meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">
                              @import url(https://themes.googleusercontent.com/fonts/css?kit=D51V29r3BtOBcI-sslA47MI7uxzF29VHzUqoRXLrPtwLBJrCeLhaqZ1U8RkFGsAn);
                            
                              .lst-kix_list_2-6>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_2-7>li:before {
                                content: "\0025a0   "
                              }
                            
                              ul.lst-kix_list_1-0 {
                                list-style-type: none
                              }
                            
                              .lst-kix_list_2-4>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_2-5>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_2-8>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_3-0>li:before {
                                content: "\0025cf   "
                              }
                            
                              .lst-kix_list_3-1>li:before {
                                content: "\0025cb   "
                              }
                            
                              .lst-kix_list_3-2>li:before {
                                content: "\0025a0   "
                              }
                            
                              ul.lst-kix_list_3-7 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_3-8 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_1-3 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_3-1 {
                                list-style-type: none
                              }
                            
                              .lst-kix_list_3-5>li:before {
                                content: "\0025a0   "
                              }
                            
                              ul.lst-kix_list_1-4 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_3-2 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_1-1 {
                                list-style-type: none
                              }
                            
                              .lst-kix_list_3-4>li:before {
                                content: "\0025a0   "
                              }
                            
                              ul.lst-kix_list_1-2 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_3-0 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_1-7 {
                                list-style-type: none
                              }
                            
                              .lst-kix_list_3-3>li:before {
                                content: "\0025a0   "
                              }
                            
                              ul.lst-kix_list_1-8 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_3-5 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_1-5 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_3-6 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_1-6 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_3-3 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_3-4 {
                                list-style-type: none
                              }
                            
                              .lst-kix_list_3-8>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_3-6>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_3-7>li:before {
                                content: "\0025a0   "
                              }
                            
                              ul.lst-kix_list_2-8 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_2-2 {
                                list-style-type: none
                              }
                            
                              .lst-kix_list_1-0>li:before {
                                content: "\0025cf   "
                              }
                            
                              ul.lst-kix_list_2-3 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_2-0 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_2-1 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_2-6 {
                                list-style-type: none
                              }
                            
                              .lst-kix_list_1-1>li:before {
                                content: "\0025cb   "
                              }
                            
                              .lst-kix_list_1-2>li:before {
                                content: "\0025a0   "
                              }
                            
                              ul.lst-kix_list_2-7 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_2-4 {
                                list-style-type: none
                              }
                            
                              ul.lst-kix_list_2-5 {
                                list-style-type: none
                              }
                            
                              .lst-kix_list_1-3>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_1-4>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_1-7>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_1-5>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_1-6>li:before {
                                content: "\0025a0   "
                              }
                            
                              li.li-bullet-0:before {
                                margin-left: -18pt;
                                white-space: nowrap;
                                display: inline-block;
                                min-width: 18pt
                              }
                            
                              .lst-kix_list_2-0>li:before {
                                content: "\0025cf   "
                              }
                            
                              .lst-kix_list_2-1>li:before {
                                content: "\0025cb   "
                              }
                            
                              .lst-kix_list_1-8>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_2-2>li:before {
                                content: "\0025a0   "
                              }
                            
                              .lst-kix_list_2-3>li:before {
                                content: "\0025a0   "
                              }
                            
                              ol {
                                margin: 0;
                                padding: 0
                              }
                            
                              table td,
                              table th {
                                padding: 0
                              }
                            
                              .c5 {
                                border-right-style: solid;
                                padding: 6pt 9pt 6pt 9pt;
                                border-bottom-color: #000000;
                                border-top-width: 1pt;
                                border-right-width: 1pt;
                                border-left-color: #000000;
                                vertical-align: top;
                                border-right-color: #000000;
                                border-left-width: 1pt;
                                border-top-style: solid;
                                background-color: #f8fafd;
                                border-left-style: solid;
                                border-bottom-width: 1pt;
                                width: 78pt;
                                border-top-color: #000000;
                                border-bottom-style: solid
                              }
                            
                              .c7 {
                                border-right-style: solid;
                                padding: 6pt 9pt 6pt 9pt;
                                border-bottom-color: #000000;
                                border-top-width: 1pt;
                                border-right-width: 1pt;
                                border-left-color: #000000;
                                vertical-align: top;
                                border-right-color: #000000;
                                border-left-width: 1pt;
                                border-top-style: solid;
                                background-color: #f8fafd;
                                border-left-style: solid;
                                border-bottom-width: 1pt;
                                width: 93.6pt;
                                border-top-color: #000000;
                                border-bottom-style: solid
                              }
                            
                              .c0 {
                                color: #1b1c1d;
                                font-weight: 700;
                                text-decoration: none;
                                vertical-align: baseline;
                                font-size: 12pt;
                                font-family: "Google Sans Text";
                                font-style: normal
                              }
                            
                              .c21 {
                                color: #1b1c1d;
                                font-weight: 700;
                                text-decoration: none;
                                vertical-align: baseline;
                                font-size: 16pt;
                                font-family: "Google Sans Text";
                                font-style: normal
                              }
                            
                              .c19 {
                                color: #1b1c1d;
                                font-weight: 700;
                                text-decoration: none;
                                vertical-align: baseline;
                                font-size: 15pt;
                                font-family: "Google Sans Text";
                                font-style: normal
                              }
                            
                              .c2 {
                                color: #1b1c1d;
                                font-weight: 400;
                                text-decoration: none;
                                vertical-align: baseline;
                                font-size: 10pt;
                                font-family: "Google Sans Text";
                                font-style: normal
                              }
                            
                              .c1 {
                                color: #1b1c1d;
                                font-weight: 400;
                                text-decoration: none;
                                vertical-align: baseline;
                                font-size: 12pt;
                                font-family: "Google Sans Text";
                                font-style: normal
                              }
                            
                              .c8 {
                                margin-left: 23.2pt;
                                padding-top: 6pt;
                                padding-bottom: 6pt;
                                line-height: 1.149999976158142;
                                padding-left: 0pt;
                                text-align: left
                              }
                            
                              .c22 {
                                font-size: 10pt;
                                font-family: "Google Sans Text";
                                font-style: italic;
                                color: #1b1c1d;
                                font-weight: 400
                              }
                            
                              .c20 {
                                font-size: 10pt;
                                font-family: "Google Sans Text";
                                font-style: normal;
                                color: #1b1c1d;
                                font-weight: 400
                              }
                            
                              .c16 {
                                padding-top: 0pt;
                                padding-bottom: 0pt;
                                line-height: 1.15;
                                text-align: left;
                                height: 11pt
                              }
                            
                              .c13 {
                                font-size: 12pt;
                                font-family: "Google Sans Text";
                                font-style: normal;
                                color: #1b1c1d;
                                font-weight: 400
                              }
                            
                              .c15 {
                                font-size: 12pt;
                                font-family: "Google Sans Text";
                                font-style: italic;
                                color: #1b1c1d;
                                font-weight: 400
                              }
                            
                              .c9 {
                                font-size: 10pt;
                                font-family: "Google Sans Text";
                                font-style: normal;
                                color: #575b5f;
                                font-weight: 400
                              }
                            
                              .c4 {
                                padding-top: 0pt;
                                padding-bottom: 0pt;
                                line-height: 1.149999976158142;
                                text-align: left
                              }
                            
                              .c23 {
                                padding-top: 6pt;
                                padding-bottom: 6pt;
                                line-height: 1.149999976158142;
                                text-align: left
                              }
                            
                              .c14 {
                                padding-top: 6pt;
                                padding-bottom: 12pt;
                                line-height: 1.149999976158142;
                                text-align: left
                              }
                            
                              .c3 {
                                padding-top: 0pt;
                                padding-bottom: 12pt;
                                line-height: 1.149999976158142;
                                text-align: left
                              }
                            
                              .c6 {
                                padding-top: 0pt;
                                padding-bottom: 6pt;
                                line-height: 1.149999976158142;
                                text-align: left
                              }
                            
                              .c18 {
                                padding-top: 24pt;
                                padding-bottom: 6pt;
                                line-height: 1.149999976158142;
                                text-align: left
                              }
                            
                              .c11 {
                                border-spacing: 0;
                                border-collapse: collapse;
                                margin-right: auto
                              }
                            
                              .c24 {
                                background-color: #ffffff;
                                max-width: 468pt;
                                padding: 72pt 72pt 72pt 72pt
                              }
                            
                              .c17 {
                                margin-left: 23.2pt;
                                padding-left: 0pt
                              }
                            
                              .c12 {
                                padding: 0;
                                margin: 0
                              }
                            
                              .c10 {
                                height: 0pt
                              }
                            
                              .title {
                                padding-top: 24pt;
                                color: #000000;
                                font-weight: 700;
                                font-size: 36pt;
                                padding-bottom: 6pt;
                                font-family: "Arial";
                                line-height: 1.0;
                                page-break-after: avoid;
                                text-align: left
                              }
                            
                              .subtitle {
                                padding-top: 18pt;
                                color: #666666;
                                font-size: 24pt;
                                padding-bottom: 4pt;
                                font-family: "Georgia";
                                line-height: 1.0;
                                page-break-after: avoid;
                                font-style: italic;
                                text-align: left
                              }
                            
                              li {
                                color: #000000;
                                font-size: 11pt;
                                font-family: "Arial"
                              }
                            
                              p {
                                margin: 0;
                                color: #000000;
                                font-size: 11pt;
                                font-family: "Arial"
                              }
                            
                              h1 {
                                padding-top: 12pt;
                                color: #000000;
                                font-weight: 700;
                                font-size: 24pt;
                                padding-bottom: 12pt;
                                font-family: "Arial";
                                line-height: 1.0;
                                text-align: left
                              }
                            
                              h2 {
                                padding-top: 11.2pt;
                                color: #000000;
                                font-weight: 700;
                                font-size: 18pt;
                                padding-bottom: 11.2pt;
                                font-family: "Arial";
                                line-height: 1.0;
                                text-align: left
                              }
                            
                              h3 {
                                padding-top: 12pt;
                                color: #000000;
                                font-weight: 700;
                                font-size: 14pt;
                                padding-bottom: 12pt;
                                font-family: "Arial";
                                line-height: 1.0;
                                text-align: left
                              }
                            
                              h4 {
                                padding-top: 12.8pt;
                                color: #000000;
                                font-weight: 700;
                                font-size: 12pt;
                                padding-bottom: 12.8pt;
                                font-family: "Arial";
                                line-height: 1.0;
                                text-align: left
                              }
                            
                              h5 {
                                padding-top: 12.8pt;
                                color: #000000;
                                font-weight: 700;
                                font-size: 9pt;
                                padding-bottom: 12.8pt;
                                font-family: "Arial";
                                line-height: 1.0;
                                text-align: left
                              }
                            
                              h6 {
                                padding-top: 18pt;
                                color: #000000;
                                font-weight: 700;
                                font-size: 8pt;
                                padding-bottom: 18pt;
                                font-family: "Arial";
                                line-height: 1.0;
                                text-align: left
                              }
                            </style>
                            </head>
                            
                            <body class="c24 doc-content">
                              <h1 class="c6"><span class="c21">A Comprehensive Course Outline: Prompt Engineering for Gemini</span></h1>
                              <p class="c3"><span class="c1">This report presents a detailed, expert-level curriculum for a course on Prompt
                                  Engineering specifically tailored for Google&#39;s Gemini models. The curriculum is structured to guide learners
                                  from foundational theories to advanced techniques and practical, hands-on applications, integrating current
                                  research and Google&#39;s official documentation.</span></p>
                              <h2 class="c6"><span class="c19">I. Introduction to Prompt Engineering</span></h2>
                              <h3 class="c6"><span class="c0">Definition and Purpose of Prompt Engineering in LLMs</span></h3>
                              <p class="c3"><span class="c1">Prompt engineering is formally defined as the systematic process of designing and
                                  optimizing input prompts to effectively guide the responses of Large Language Models (LLMs). This methodical
                                  approach aims to ensure that the generated output exhibits high levels of accuracy, relevance, coherence, and
                                  overall usability.[1] A central objective of this discipline is to enhance the functional effectiveness of LLMs
                                  without requiring any modification to their underlying core model parameters.[1, 2] This characteristic allows for
                                  the seamless integration of pre-trained models into a wide array of downstream tasks, as the desired model
                                  behaviors are elicited purely through the carefully constructed prompt.[2]</span></p>
                              <p class="c3"><span class="c1">This approach signifies a fundamental transformation in how AI models are utilized.
                                  Rather than engaging in resource-intensive model retraining or extensive fine-tuning for each new task, prompt
                                  engineering offers an agile, instruction-based method for adaptation. This capability to achieve significant
                                  performance enhancements without altering the foundational model parameters is a key departure from conventional
                                  AI development. It means that developers can rapidly deploy and customize powerful pre-trained models for novel
                                  applications, circumventing the substantial computational expense, extensive data requirements, and considerable
                                  time investment typically associated with full retraining or deep fine-tuning. This agility and efficiency in
                                  resource utilization elevate prompt engineering to a strategic capability, broadening access to advanced AI
                                  technologies and accelerating the pace of application development across various industries. The focus shifts from
                                  altering the internal architecture of the model to refining the interaction interface with the model, essentially
                                  how to think and communicate with AI effectively.</span></p>
                              <h3 class="c6"><span class="c0">Historical Context and Evolution</span></h3>
                              <p class="c3"><span class="c1">The domain of prompt engineering has undergone a significant evolution, tracing its
                                  origins back to early rule-based inputs in the 1950s. Its progression continued through key milestones in machine
                                  learning and deep learning, experiencing a notable acceleration post-2017 with the advent of transformer
                                  architectures and models such as GPT.[1] What began largely as an empirical practice, relying on trial-and-error,
                                  has since matured into a well-structured research domain, extending its influence across numerous
                                  disciplines.[1]</span></p>
                              <p class="c3"><span class="c1">The rapid formalization of prompt engineering into a rigorous research field
                                  underscores its increasing complexity and the necessity for systematic investigation beyond anecdotal best
                                  practices. This evolution implies that initial prompt design efforts were largely experimental. However, as LLMs
                                  became more sophisticated and their impact on various applications expanded, the demand for reproducible,
                                  theoretically grounded methodologies became paramount. The development of advanced techniques, such as
                                  Chain-of-Thought (CoT), Tree-of-Thought (ToT), and the establishment of formal evaluation metrics, represents a
                                  direct consequence of this formalization. This progression indicates that prompt engineering is transitioning from
                                  being primarily an &quot;art&quot; to increasingly becoming a &quot;science,&quot; demanding rigorous
                                  methodologies and a deeper understanding of the internal mechanisms of LLMs. This shift profoundly influences the
                                  structure of educational programs, moving beyond simple &quot;tips and tricks&quot; to encompass fundamental
                                  principles and systematic approaches.</span></p>
                              <h3 class="c6"><span class="c0">Why Prompt Engineering is Crucial for Gemini</span></h3>
                              <p class="c3"><span class="c1">Prompt engineering holds particular importance for Google&#39;s Gemini models due to
                                  their inherent design. Gemini models are engineered with massive context capabilities, enabling powerful
                                  in-context learning.[3] This architectural strength necessitates effective prompt engineering to fully exploit
                                  features like its expansive 1 million token context window and native multimodal understanding.[3] Achieving
                                  precise and desirable output is essential, especially as generative AI models transition from experimental
                                  novelties to integral tools embedded within real-world products and services.[4]</span></p>
                              <p class="c3"><span class="c1">Gemini&#39;s unique architectural advantages, specifically its vast context window and
                                  native multimodality, elevate prompt engineering from a mere optimization technique to a foundational requirement
                                  for unlocking the model&#39;s complete potential. The ability of Gemini to process extensive amounts of
                                  information directly, without relying on external Retrieval Augmented Generation (RAG) systems or sliding window
                                  techniques [3], means that the quality and structure of the input prompt become critically important. If the model
                                  can learn effectively from hundreds of thousands of examples provided &quot;in-context&quot; [3], then the
                                  strategic design of prompts to efficiently provide these examples and instructions is no longer optional but
                                  indispensable. This implies that for Gemini, prompt engineering is not simply about generating any response; it is
                                  about consistently achieving precise, high-quality, and contextually relevant outputs that fully leverage its
                                  advanced capabilities. The discipline thus moves beyond basic interaction to encompass the strategic architecture
                                  of information within the prompt itself.</span></p>
                              <h2 class="c6"><span class="c19">II. Module 1: Foundational Prompt Engineering Concepts</span></h2>
                              <h3 class="c6"><span class="c0">Core Concepts</span></h3>
                              <h4 class="c6"><span class="c0">Understanding LLM Interaction Principles</span></h4>
                              <p class="c3"><span class="c1">Large Language Models process input text by encoding it into high-dimensional vector
                                  representations, subsequently generating responses in an autoregressive manner.[1] The quality of the output is
                                  profoundly influenced by two primary factors: the design of the prompt, which establishes the context and
                                  specificity of the request, and the configuration of model hyperparameters.[1] Generative models typically produce
                                  text through a two-stage process. The first stage is deterministic, involving the processing of the input prompt
                                  and the generation of a probability distribution over all possible next tokens. The second stage is stochastic,
                                  where the model converts these probability distributions into actual text responses by employing various decoding
                                  strategies.[5]</span></p>
                              <p class="c3"><span class="c1">The inherent deterministic-stochastic nature of LLM response generation suggests that
                                  prompt design primarily influences the deterministic probability distribution of potential outputs. Conversely,
                                  parameters such as temperature, Top-K, and Top-P govern the stochastic sampling process, offering distinct control
                                  points for shaping the behavioral characteristics of the output. This two-stage process establishes a clear
                                  division of control. The prompt, with its embedded context and instructions, fundamentally shapes what the model
                                  perceives as probable outcomes, influencing the underlying probability distribution. The parameters, including
                                  temperature, Top-K, and Top-P, then dictate how this probability distribution is sampled to produce the final
                                  output, thereby controlling the degree of randomness or creativity. For prompt engineers, this understanding means
                                  that if the desired content or meaning is not appearing in the output, the prompt itself requires refinement,
                                  addressing the deterministic stage. If the content is correct but the style, diversity, or length is suboptimal,
                                  then adjusting the model parameters becomes the key, addressing the stochastic stage. This distinction enables
                                  more targeted and efficient debugging and optimization of prompt outputs.</span></p>
                              <h4 class="c6"><span class="c0">Prompt Structure: Instructions, Context, Examples, Input Types</span></h4>
                              <p class="c3"><span class="c1">Prompts are versatile constructs that can incorporate various types of content,
                                  including questions, explicit instructions, contextual information, illustrative examples, and partial inputs for
                                  the model to complete.[6] Effective prompts are characterized by their clear and specific instructions [5], their
                                  ability to impose constraints on the model&#39;s behavior [5], and their specification of the desired response
                                  format.[5] The inclusion of contextual information is crucial for helping the model comprehend the specific
                                  constraints and intricate details of a request.[6] Examples, particularly in few-shot prompting, serve as
                                  input-output pairs that demonstrate an ideal response pattern.[5, 6] Common input types for prompts include
                                  questions, tasks, entities, and completion-based inputs.[5, 6]</span></p>
                              <p class="c3"><span class="c1">The modularity inherent in prompt structure, encompassing instructions, context, and
                                  examples, facilitates a systematic, layered approach to prompt design. This modularity allows for fine-grained
                                  control over the model&#39;s behavior, particularly for complex tasks. This is not merely a list of elements but a
                                  functional decomposition. Instructions define the core objective, context provides the boundaries and scope of the
                                  task, and examples illustrate the desired patterns or formats for the output. This modularity suggests that for
                                  intricate tasks, prompt engineers can systematically construct a prompt: initially defining the primary
                                  instruction, then injecting relevant context to narrow the operational domain, and finally providing examples to
                                  guide specific formatting or reasoning patterns. This structured methodology enhances prompt clarity, minimizes
                                  ambiguity, and contributes to more robust and debuggable prompts, akin to designing well-defined software
                                  modules.</span></p>
                              <h4 class="c6"><span class="c0">Model Parameters: Temperature, Top-K, Top-P, Max Output Tokens, Stop Sequences</span>
                              </h4>
                              <p class="c3"><span class="c13">Several model parameters play a critical role in controlling the generation of
                                  responses.[5, 7] </span><span class="c9">Temperature</span><span class="c13">&nbsp;is a key parameter that
                                  modulates the balance between randomness and determinism in the output; higher values increase the diversity and
                                  creativity of responses, while lower values lead to more deterministic outputs.[1, 5, 7] A temperature setting of
                                  0 typically results in highly deterministic responses.[5] </span><span class="c9">Top-K</span><span
                                  class="c13">&nbsp;restricts the model&#39;s token selection to the </span><span class="c15">k</span><span
                                  class="c13">&nbsp;most probable tokens at each step.[1, 7] </span><span class="c9">Top-P</span><span
                                  class="c13">&nbsp;selects tokens from most to least probable until their cumulative probabilities sum to the
                                  specified </span><span class="c9">Top-P</span><span class="c13">&nbsp;value.[5, 7] </span><span class="c9">Max
                                  output tokens</span><span class="c13">&nbsp;defines the maximum permissible length of the generated response.[5,
                                  7] Finally, </span><span class="c9">Stop sequences</span><span class="c1">&nbsp;are specific character sequences
                                  that, when encountered by the model, halt the response generation process.[7]</span></p>
                              <p class="c3"><span class="c13">The interplay between temperature, Top-K, and Top-P offers a nuanced mechanism for
                                  controlling the creativity-to-fidelity spectrum of LLM outputs. This allows engineers to precisely tune responses
                                  for diverse application requirements. Temperature directly scales the log-probabilities of tokens before sampling,
                                  increasing the likelihood of less probable tokens being selected at higher settings. Top-K and Top-P, conversely,
                                  act as filters </span><span class="c15">after</span><span class="c1">&nbsp;the temperature adjustment, narrowing
                                  the pool of tokens from which the model samples. Top-K enforces a fixed count of tokens, while Top-P dynamically
                                  considers a cumulative probability mass. This means that temperature broadly establishes the overall level of
                                  creativity or randomness, while Top-K and Top-P provide precise control over the vocabulary and coherence within
                                  that established randomness. For example, a high temperature combined with a low Top-P value might generate
                                  diverse yet coherent text, whereas a high temperature without Top-P or Top-K constraints could lead to more
                                  erratic or less relevant output. Understanding this hierarchical relationship enables targeted tuning: if the
                                  output is too generic, increasing the temperature may be beneficial; if it is too unconstrained, reducing
                                  temperature or applying Top-P/Top-K constraints can improve coherence.</span></p>
                              <h3 class="c6"><span class="c0">Basic Prompting Techniques</span></h3>
                              <h4 class="c6"><span class="c0">Zero-Shot Prompting: Principles and Applications</span></h4>
                              <p class="c3"><span class="c1">Zero-shot prompting represents a significant advancement in the utilization of LLMs.
                                  This technique enables models to address novel tasks without the need for extensive training data or explicit
                                  input-output examples.[2, 8, 9] The model relies solely on the information provided within the prompt and its
                                  pre-existing knowledge base to generate predictions for the new task.[2] This method has demonstrably improved the
                                  performance of LLMs across various applications.[9]</span></p>
                              <p class="c3"><span class="c1">The surprising effectiveness of certain zero-shot prompts, such as the simple phrase
                                  &quot;Let&#39;s think step-by-step,&quot; points to the presence of emergent reasoning capabilities within large
                                  models. This suggests that straightforward linguistic cues can unlock complex internal processing mechanisms. If a
                                  basic instruction like &quot;Let&#39;s think step-by-step&quot; can significantly enhance performance on reasoning
                                  tasks without any explicit examples (as seen in zero-shot CoT), it implies that the LLM already possesses the
                                  underlying knowledge and capacity to perform these intermediate steps. The prompt, in this context, functions as a
                                  trigger or a directive to reveal this internal process. This indicates that prompt engineering for zero-shot tasks
                                  is less about providing explicit examples and more about identifying the optimal meta-instructions that align the
                                  model&#39;s internal cognitive processes with the desired task. This direction encourages further exploration into
                                  &quot;prompt interpretability,&quot; as exemplified by metrics like the ZIP score [9], to understand precisely why
                                  certain words or phrases exert a more substantial influence on model output, leading to more principled zero-shot
                                  prompt design.</span></p>
                              <h4 class="c6"><span class="c0">Few-Shot Prompting: Principles, Example Selection, and Formatting</span></h4>
                              <p class="c3"><span class="c1">Few-shot prompting involves supplying models with a small collection of input-output
                                  examples to help them comprehend a given task.[2, 8] Even a limited number of high-quality examples can
                                  substantially improve model performance, particularly on complex tasks.[2] This technique is frequently employed
                                  to regulate the formatting, phrasing, scope, or general patterning of generated responses.[5] Maintaining
                                  consistent formatting across these examples is crucial to prevent the model from generating undesirable output
                                  structures.[5] However, it is important to note that providing an excessive number of examples can potentially
                                  lead to overfitting.[5]</span></p>
                              <p class="c3"><span class="c13">Few-shot prompting, while highly effective for enabling pattern recognition and
                                  controlling output formatting, introduces a trade-off between performance gains and practical constraints such as
                                  token limits, the potential for overfitting, and the introduction of biases. The benefits, including improved
                                  performance and controlled output, come at the cost of increased token usage, the possibility of subtle biases
                                  stemming from example selection, and the risk of overfitting if the examples are not sufficiently diverse or are
                                  too numerous. This means that few-shot prompting necessitates careful optimization. The effectiveness is not
                                  solely dependent on </span><span class="c15">what</span><span class="c13">&nbsp;examples are provided, but also on
                                </span><span class="c15">how many</span><span class="c13">, </span><span class="c15">how diverse</span><span
                                  class="c13">, and </span><span class="c15">how consistently they are formatted</span><span class="c13">. This
                                  often requires iterative testing and potentially automated strategies for example selection to maximize the
                                  benefits while mitigating potential drawbacks, especially when utilizing models like Gemini, which possess
                                  extensive context windows where many examples </span><span class="c15">could</span><span class="c1">&nbsp;be
                                  provided [3] but should only be included after careful consideration.</span></p>
                              <h4 class="c6"><span class="c0">Instruction-Based Prompting: Crafting Clear and Precise Directives</span></h4>
                              <p class="c3"><span class="c1">Instruction-based prompting involves the meticulous design of directives that
                                  explicitly guide the model&#39;s behavior and output.[1] The clarity and precision of these instructions are
                                  paramount for ensuring that the model performs tasks as intended, thereby avoiding ambiguity and
                                  misinterpretations.[1] Conversely, poorly designed or overly generalized instructions often result in output that
                                  lacks specificity and relevance.[1] Fundamentally, prompts function as natural language instructions that provide
                                  the necessary context for the model to operate.[2]</span></p>
                              <p class="c3"><span class="c1">The efficacy of instruction-based prompting is rooted in the principle of
                                  &quot;precision in ambiguity,&quot; where the clarity of the instruction directly correlates with the specificity
                                  and relevance of the model&#39;s output. This establishes a direct cause-and-effect relationship: clear
                                  instructions lead to desired, specific outputs, whereas ambiguous instructions result in generic or irrelevant
                                  responses. The model functions as a highly capable instruction-follower but cannot infer unstated intent. This
                                  means that prompt engineers must dedicate substantial effort to crafting instructions that are not only
                                  grammatically correct but also semantically unambiguous. This often entails breaking down complex tasks into
                                  simpler, more direct directives, employing explicit constraints, and clearly defining desired output formats,
                                  effectively &quot;programming&quot; the LLM through natural language.</span></p>
                              <h4 class="c6"><span class="c0">Role-Based (Persona) Prompting: Assigning Identity and its Impact</span></h4>
                              <p class="c3"><span class="c1">Assigning specific personas or roles to LLMs has been shown to enhance their reasoning
                                  capabilities, particularly when the chosen persona is domain-specific.[4, 10] However, this technique is akin to a
                                  &quot;double-edged sword&quot; [10], as it also carries the significant risk of increasing the generation of
                                  harmful content and amplifying toxicity.[4, 11] Furthermore, it can introduce biases if the assigned role is not
                                  strongly aligned with the specific question or task.[10]</span></p>
                              <p class="c3"><span class="c1">Persona prompting presents a critical ethical and practical dilemma: while it
                                  demonstrably improves domain-specific reasoning, it concurrently amplifies the risk of bias and the generation of
                                  harmful content, thereby necessitating robust mitigation strategies. There is a clear tension between the utility
                                  of enhanced reasoning and the inherent risks of increased toxicity or bias. The model&#39;s &quot;role-playing
                                  ability&quot; appears to extend to adopting negative characteristics or biases associated with a given persona.
                                  This mandates that any application employing persona prompting must integrate strong safety filters and adhere to
                                  stringent ethical considerations. It also suggests a future direction for research focused on
                                  &quot;de-biasing&quot; persona prompts or developing methods to leverage the reasoning benefits without inheriting
                                  negative traits, potentially through multi-model feedback strategies [11] or ensemble approaches.[10]</span></p>
                              <h3 class="c6"><span class="c0">Prompt Design Principles</span></h3>
                              <h4 class="c6"><span class="c0">Clarity, Specificity, and Conciseness</span></h4>
                              <p class="c3"><span class="c1">These are fundamental attributes for designing effective instructions, ensuring that
                                  the model accurately interprets and executes the given task.[1, 5]</span></p>
                              <h4 class="c6"><span class="c0">Adding Constraints and Desired Response Formats</span></h4>
                              <p class="c3"><span class="c1">Prompt engineers can specify explicit constraints on how the model should interpret the
                                  prompt or generate its response. This includes instructing the model on what to do and what to avoid, such as
                                  limiting the length of a summary or requiring a specific output format like a table or a bulleted list.[5]</span>
                              </p>
                              <h4 class="c6"><span class="c0">Leveraging Prefixes (Input, Output, Example)</span></h4>
                              <p class="c3"><span class="c1">Prefixes are words or phrases added to prompt content that serve various purposes
                                  depending on their placement. For instance, an input prefix like &quot;English:&quot; or &quot;French:&quot; can
                                  signal semantically distinct parts of the input to the model. An output prefix, such as &quot;JSON:&quot;, informs
                                  the model about the expected format of the response. In few-shot prompts, example prefixes provide labels that aid
                                  the model in parsing and understanding the output structure.[5]</span></p>
                              <h4 class="c6"><span class="c0">First Principles Thinking for Prompt Design</span></h4>
                              <p class="c6"><span class="c1">This approach to prompt engineering involves breaking down complex problems into their
                                  fundamental components and building solutions from the ground up, rather than relying on analogies or previous
                                  solutions. Applied to prompting, this means systematically defining:</span></p>
                              <ul class="c12 lst-kix_list_1-0 start">
                                <li class="c6 c17 li-bullet-0"><span class="c1">Goal State: Clearly articulate the desired outcome or final state of
                                    the model&#39;s response. What does success look like? (Connects to Clarity, Specificity, and
                                    Conciseness)</span></li>
                                <li class="c8 li-bullet-0"><span class="c13">Source Material: Identify all necessary input data, context, or
                                    information the model needs to process. This includes what the model should </span><span
                                    class="c15">know</span><span class="c13">&nbsp;or </span><span class="c15">refer to</span><span class="c1">.
                                    (Connects to Prompt Structure: Context and Leveraging Gemini&#39;s Long Context Window)</span></li>
                                <li class="c8 li-bullet-0"><span class="c1">Constraints: Define any limitations, rules, or boundaries the model must
                                    adhere to in its generation. This includes length, format, tone, or content restrictions. (Directly connects to
                                    Adding Constraints and Desired Response Formats)</span></li>
                                <li class="c8 li-bullet-0"><span class="c1">Process Instructions: Outline the step-by-step procedure or reasoning
                                    path the model should follow to achieve the Goal State using the Source Material and respecting the Constraints.
                                    (Connects to Instruction-Based Prompting and Chain-of-Thought)</span></li>
                                <li class="c8 li-bullet-0"><span class="c1">Validation Signals: Specify what constitutes a &quot;great&quot;
                                    response. This can include examples of desired output, a checklist of criteria, or the required format. This
                                    helps the model understand the quality bar. (Connects to Few-Shot Prompting and Iterative Prompt
                                    Refinement)</span></li>
                                <li class="c8 li-bullet-0"><span class="c1">Iteration Plan: Establish a strategy for refining the prompt based on
                                    initial outputs, feedback, and performance evaluation. This acknowledges that prompt design is an iterative
                                    process. (Directly connects to Iterative Prompt Refinement Strategies)</span></li>
                              </ul>
                              <p class="c14"><span class="c1">By applying First Principles Thinking, prompt engineers can construct more robust,
                                  precise, and effective prompts, especially for novel or highly complex tasks, by systematically addressing each
                                  fundamental aspect of the desired AI interaction.</span></p>
                              <h4 class="c6"><span class="c0">Iterative Prompt Refinement Strategies</span></h4>
                              <p class="c3"><span class="c1">Prompt design is inherently an iterative process, requiring continuous adjustment and
                                  optimization.[5] Effective strategies for refinement include experimenting with different phrasing, switching to
                                  analogous tasks if the initial approach is ineffective, and altering the order of content within the prompt.[5]
                                  This systematic process of refinement, which mirrors how human machine learning experts iteratively improve models
                                  by focusing on one component at a time, demonstrably enhances overall performance and stability.[12, 13] The
                                  Iteration Plan from First Principles Thinking is crucial here, providing a structured approach to this continuous
                                  improvement, often using Validation Signals as benchmarks.</span></p>
                              <p class="c3"><span class="c1">Iterative prompt refinement elevates prompt engineering from a static craft to a
                                  dynamic, data-driven optimization process, aligning it closely with traditional software development and machine
                                  learning lifecycles. This suggests that prompts are not static artifacts but require continuous testing,
                                  evaluation, and refinement based on performance feedback, much like software is iteratively developed and models
                                  is iteratively trained. The use of a &quot;training set derived from existing datasets&quot; [13] for prompt
                                  refinement further solidifies this analogy. This implies that successful prompt engineering demands a development
                                  mindset, encompassing methodologies for A/B testing prompts, version control for prompt iterations, and
                                  potentially the adoption of automated prompt optimization tools, such as Prochemy.[13] The focus thus shifts from
                                  discovering a single &quot;perfect prompt&quot; upfront to establishing a robust &quot;prompt development
                                  pipeline.&quot;</span></p>
                              <h3 class="c6"><span class="c0">Table 1: Core Prompting Techniques &amp; Gemini Relevance</span></h3>
                              <table class="c11"></table>
                              <p class="c16"><span class="c0"></span></p>
                              <table class="c11">
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Prompt Type</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Description</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Basic Example</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">When to Use (General)</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Common Mistake</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Gemini-Specific Considerations/Benefits</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Zero-shot</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Direct task instruction with no examples. Model relies on pre-existing
                                        knowledge.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">&quot;Write a product description for a Bluetooth speaker.&quot;</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Simple, general tasks where the model has high confidence or emergent
                                        capabilities are desired.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Too vague or general, e.g., &quot;Describe this.&quot;</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Handles clean instructions well; specific cues like &quot;Let&#39;s think
                                        step-by-step&quot; can unlock complex abilities.[4, 9]</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Few-shot</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Provides a small number of input-output examples to teach a pattern or
                                        behavior.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">&quot;Translate: Bonjour &rarr; Hello. Merci &rarr; Thank you.&quot;</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Teaching tone, reasoning, classification, or desired output format.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Using inconsistent or overly complex examples; too many examples leading to
                                        overfitting.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Benefits from consistent formatting and varied examples; can leverage long
                                        context for more examples, but optimal number is key to avoid overfitting.[5]</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Instruction-based</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Explicit directives guiding model behavior and output.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">&quot;Summarize the text below in 100 words, focusing on key
                                        arguments.&quot;</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Ensuring precise task execution, avoiding ambiguity, and controlling
                                        specificity.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Overly general instructions lacking specificity or desired constraints.</span>
                                    </p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Responds well to clear, precise directives; enables fine-grained control over
                                        output.[1, 5]</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Role-based (Persona)</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Assigns a specific identity, context, or behavioral frame to the model.</span>
                                    </p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">&quot;You are an AI policy advisor. Draft a summary.&quot;</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Tasks requiring specific tone, domain expertise, or simulated perspective.</span>
                                    </p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Not specifying how the role should influence behavior; introducing unintended
                                        biases.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Role clarity helps guide tone and content; requires careful consideration of
                                        potential for increased toxicity or bias.[4, 10, 11]</span></p>
                                  </td>
                                </tr>
                              </table>
                              <h3 class="c18"><span class="c0">Table 2: Gemini Model Parameters &amp; Impact</span></h3>
                              <table class="c11"></table>
                              <p class="c16"><span class="c0"></span></p>
                              <table class="c11">
                                <tr class="c10">
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Parameter</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Description</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Range/Typical Values</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Impact on Output</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Best Practices for Gemini</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Temperature</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Controls the degree of randomness in token selection.</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">0.0 - 1.0 (or higher, model-dependent)</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Higher values increase diversity/creativity; lower values lead to more
                                        deterministic/focused responses. A value of 0.0 makes output mostly deterministic.</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Increase for creative tasks; decrease for factual or precise tasks. If responses
                                        are too generic, try increasing.[1, 5, 7]</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Top-K</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c20">Restricts token choices to the </span><span class="c22">k</span><span
                                        class="c2">&nbsp;most probable tokens at each step.</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Integer (e.g., 1-40)</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Narrows the pool of possible tokens, enhancing coherence and contextual
                                        relevance.</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Use with temperature for controlled randomness. A value of 1 selects the most
                                        probable token (greedy decoding).[1, 7]</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Top-P</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c20">Selects tokens from most to least probable until their cumulative probabilities
                                        sum to the </span><span class="c9">top-P</span><span class="c2">&nbsp;value.</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">0.0 - 1.0</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Provides a dynamic way to filter token choices, adapting to the probability
                                        distribution.</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Use with temperature for more refined control over diversity. For least variable
                                        results, set to 0.[5, 7]</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Max Output Tokens</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Specifies the maximum number of tokens generated in the response.</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Integer (e.g., 1-2048)</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Controls the length of the generated output. (100 tokens &asymp; 60-80
                                        words).</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Set appropriately for desired response length. Lower for shorter, higher for
                                        potentially longer responses.[5, 7]</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Stop Sequences</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">A series of characters that, when encountered, cause the model to stop generating
                                        content.</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Custom string(s) (up to 5)</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Ensures responses terminate at specific points, useful for structured outputs or
                                        preventing unwanted continuation.</span></p>
                                  </td>
                                  <td class="c7" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Define clear stop sequences to control response boundaries, especially in
                                        multi-turn interactions or structured data generation.[7]</span></p>
                                  </td>
                                </tr>
                              </table>
                              <h2 class="c18"><span class="c19">III. Module 2: Advanced Prompting Techniques</span></h2>
                              <h3 class="c6"><span class="c0">Chain-of-Thought (CoT) Prompting</span></h3>
                              <h4 class="c6"><span class="c0">Facilitating Step-by-Step Reasoning and &quot;Building Things Back Up&quot;</span>
                              </h4>
                              <p class="c3"><span class="c1">Chain-of-Thought (CoT) prompting is a technique designed to guide Large Language Models
                                  through coherent, step-by-step reasoning processes. This approach is particularly effective in bridging the
                                  performance gap for complex reasoning tasks where LLMs might otherwise struggle.[2, 14] It emulates the human
                                  cognitive process of breaking down intricate problems into a series of logical intermediate steps, and then
                                  &quot;building things back up&quot; by integrating these steps to form a comprehensive solution.[2]</span></p>
                              <p class="c3"><span class="c13">CoT prompting reveals that LLMs possess latent reasoning capabilities that are not
                                  always spontaneously expressed but can be activated by explicit meta-instructions or structured examples. This
                                  fundamentally alters the perception of their underlying intelligence. If a simple phrase like &quot;Let&#39;s
                                  think step-by-step&quot; can significantly improve performance on reasoning tasks without explicit examples (as
                                  observed in zero-shot CoT), it implies that the LLM already holds the necessary knowledge and capacity to perform
                                  these steps. The prompt, in this context, functions as a trigger or a directive to reveal this internal process.
                                  This suggests that LLMs are not merely sophisticated autocomplete engines but harbor a deeper, albeit opaque,
                                  internal representation of knowledge and reasoning. Prompt engineering, particularly through CoT, becomes a method
                                  of &quot;interrogating&quot; or &quot;guiding&quot; these internal processes, pushing the boundaries of what LLMs
                                  can achieve in complex problem-solving by eliciting a </span><span class="c15">process</span><span
                                  class="c1">&nbsp;from the model rather than just a direct answer.</span></p>
                              <h4 class="c6"><span class="c0">Zero-Shot CoT and Few-Shot CoT</span></h4>
                              <p class="c3"><span class="c1">CoT can be applied in a zero-shot context, where a simple instruction like
                                  &quot;Let&#39;s think step-by-step&quot; is appended to the query, prompting the model to generate intermediate
                                  reasoning steps before the final answer.[9] Alternatively, few-shot CoT involves providing the model with a few
                                  examples that explicitly demonstrate the desired reasoning chain, allowing it to learn the pattern for similar
                                  tasks.[14]</span></p>
                              <h4 class="c6"><span class="c0">Variants: Logical CoT, Faithful CoT</span></h4>
                              <p class="c3"><span class="c1">Beyond the basic CoT framework, specialized variants have emerged to address specific
                                  reasoning challenges. Logical Chain-of-Thought (LogiCoT) focuses on enhancing the LLM&#39;s ability to perform
                                  logical reasoning for complex, multi-step problems across diverse domains.[2] Faithful CoT, another variant,
                                  integrates both natural language explanations and symbolic reasoning, such as code, to provide a more
                                  comprehensive and verifiable reasoning path.[15]</span></p>
                              <h3 class="c6"><span class="c0">Self-Consistency (SC)</span></h3>
                              <h4 class="c6"><span class="c0">Aggregating Diverse Reasoning Paths</span></h4>
                              <p class="c3"><span class="c1">Self-Consistency (SC) is a robust strategy designed to mitigate failures in LLM outputs
                                  by aggregating multiple sampled responses. This technique operates on the premise that diverse reasoning paths,
                                  generated through stochastic sampling (e.g., by using a non-zero temperature), will converge towards correct
                                  answers when their final outputs are aggregated, often via a majority voting mechanism.[16, 17]</span></p>
                              <h4 class="c6"><span class="c0">Applications and Limitations in Long Contexts</span></h4>
                              <p class="c3"><span class="c1">SC has demonstrated high effectiveness in tasks involving short content, typically
                                  those under 100 tokens.[16] However, its performance consistently degrades when applied to long-context scenarios,
                                  such as those exceeding 10,000 tokens, across various models and tasks.[16] This performance degradation is
                                  primarily attributed to persistent positional bias within the models. In such cases, all sampled responses tend to
                                  inherit the same structural biases related to token position, which fundamentally violates SC&#39;s core
                                  assumption of error independence. Consequently, instead of mitigating errors, sampling from a model with inherent
                                  positional bias can amplify them.[16]</span></p>
                              <p class="c3"><span class="c1">The failure of Self-Consistency in long-context scenarios due to positional bias
                                  highlights a fundamental limitation of current LLMs and their aggregation strategies. This indicates that simply
                                  providing more data or a longer context does not automatically translate to improved results without context-aware
                                  methodological adjustments. The core assumption of SC is that errors across diverse samples are independent and
                                  will therefore cancel out through aggregation. However, if the errors are systematic, such as a consistent
                                  misinterpretation of information at certain positions within a long input, then aggregating these biased samples
                                  will only amplify the error rather than mitigating it. This finding is particularly critical for Gemini, given its
                                  1 million token context window. It implies that merely feeding vast amounts of context to the model and applying
                                  SC will not suffice for complex reasoning tasks over lengthy documents. This necessitates new research and
                                  development into &quot;context-aware methods, such as position-aware voting or debiased sampling&quot;.[16] For
                                  practical applications, developers must exercise caution when applying SC to long-form tasks and consider
                                  combining it with other techniques or pre-processing steps designed to mitigate positional bias.</span></p>
                              <h3 class="c6"><span class="c0">Tree-of-Thought (ToT) and Graph of Thoughts (GoT)</span></h3>
                              <h4 class="c6"><span class="c0">Exploring Multiple Reasoning Paths and Backtracking</span></h4>
                              <p class="c3"><span class="c1">Tree-of-Thought (ToT) is an advanced prompting framework that models the LLM&#39;s
                                  reasoning process as a tree structure. This framework enables lookahead and tree search strategies, allowing the
                                  model to explore multiple potential branches of reasoning before committing to a final path.[18, 19] This
                                  capability makes ToT particularly well-suited for general problem-solving and scenarios demanding complex
                                  decision-making, as it offers the crucial ability to backtrack from non-promising outcomes.[18, 19]</span></p>
                              <h4 class="c6"><span class="c0">Benefits for Complex Problem Solving</span></h4>
                              <p class="c3"><span class="c1">ToT significantly enhances the problem-solving abilities of LLMs by enabling them to
                                  explore multiple reasoning paths concurrently, thereby mirroring human cognitive processes where various potential
                                  solutions are considered before selecting the most viable one.[18] This approach has demonstrated superior
                                  performance in tasks that require strategic thinking or planning, such as solving word puzzles or generating
                                  creative writing.[18]</span></p>
                              <p class="c3"><span class="c1">Building upon ToT, the Graph of Thoughts (GoT) framework further advances prompting
                                  capabilities by modeling the information generated by an LLM as an arbitrary graph structure. In GoT, individual
                                  units of information, referred to as &quot;LLM thoughts,&quot; are represented as vertices, and the dependencies
                                  between these thoughts are represented as edges.[19] This innovative approach enables the combination of arbitrary
                                  LLM thoughts into synergistic outcomes, the distillation of the essence from entire networks of thoughts, and the
                                  enhancement of thoughts through feedback loops.[19] GoT has demonstrated superior performance compared to both CoT
                                  and ToT in terms of quality and cost-efficiency on certain tasks, for example, improving the quality of sorting by
                                  62% over ToT while simultaneously reducing costs by over 31%.[19]</span></p>
                              <p class="c3"><span class="c1">The evolution from Chain-of-Thought to Tree-of-Thought and subsequently to Graph of
                                  Thoughts illustrates a progressive effort to imbue LLMs with increasingly sophisticated, human-like, non-linear
                                  reasoning and planning capabilities. This progression&mdash;from linear steps in CoT, to tree-like exploration
                                  with branching and backtracking in ToT, and finally to arbitrary graph structures for combining thoughts and
                                  feedback loops in GoT&mdash;reflects a deeper understanding of complex reasoning. Human cognition rarely follows a
                                  simple linear path; instead, individuals explore alternatives, backtrack, combine disparate ideas, and form
                                  intricate mental models. ToT and GoT represent engineering endeavors to replicate these more advanced cognitive
                                  functions within LLMs through innovative prompt design. This advancement pushes LLMs beyond mere &quot;answer
                                  generators&quot; to become more effective &quot;problem solvers&quot; by providing them with a structured internal
                                  &quot;thought space.&quot;</span></p>
                              <h3 class="c6"><span class="c0">Iterative Refinement and Multi-Turn Prompting</span></h3>
                              <h4 class="c6"><span class="c0">Systematic Optimization of Prompts</span></h4>
                              <p class="c3"><span class="c1">Iterative Refinement is a strategic approach for designing LLM-driven machine learning
                                  pipelines, drawing inspiration from how human ML experts systematically refine models. This method involves
                                  focusing on and updating one component at a time, rather than implementing sweeping changes simultaneously.[12] By
                                  systematically modifying individual components based on real training feedback, this strategy consistently
                                  improves overall model performance.[12] Prochemy, for instance, is a specific approach that iteratively refines
                                  prompts for code generation tasks by analyzing the model&#39;s performance against a training set.[13]</span></p>
                              <p class="c3"><span class="c1">For complex tasks that involve multiple sequential steps, a highly effective strategy
                                  is to break down the overall task into individual prompts, with the output of one prompt serving as the input for
                                  the subsequent one.[5] This concept is integral to &quot;Flow Engineering,&quot; which focuses on designing
                                  multi-stage, iterative LLM workflows, contrasting with simpler, single-prompt interactions. The idea of
                                  &quot;building things back up&quot; after breaking them down is central to this chaining process.[15]</span></p>
                              <p class="c3"><span class="c1">Another advanced technique involves performing different parallel sub-tasks on distinct
                                  portions of data and then aggregating the individual results to produce a comprehensive final output.[5] This
                                  allows for distributed processing and synthesis of information.</span></p>
                              <p class="c3"><span class="c1">The shift towards multi-turn, chained, and iteratively refined prompting signifies a
                                  move beyond single-shot interactions to the design of sophisticated, programmatic LLM workflows. This effectively
                                  transforms LLMs into integral components within larger AI systems. This trend indicates a clear departure from
                                  isolated, one-off prompts, as complex real-world problems rarely fit neatly into a single prompt. Instead, they
                                  necessitate the decomposition of tasks, sequential processing, parallel execution, and continuous improvement.
                                  This transforms prompt engineering from a &quot;crafting&quot; skill into an &quot;architecting&quot; skill. It
                                  implies that future prompt engineers will not merely write individual prompts but will design entire &quot;prompt
                                  pipelines&quot; or &quot;agentic workflows&quot; where LLMs interact with each other, external tools, and feedback
                                  loops. This evolution is crucial for building robust, scalable, and intelligent applications that extend far
                                  beyond simple text generation.</span></p>
                              <h3 class="c6"><span class="c0">Metaprompting: AI as a Thinking Partner</span></h3>
                              <p class="c6"><span class="c1">Metaprompting is an advanced technique where the Large Language Model itself is
                                  prompted to assist in the design, refinement, or optimization of other prompts or workflows. This transforms the
                                  AI from a mere executor of instructions into a collaborative &quot;thinking partner&quot; in the prompt
                                  engineering process. This can involve asking the AI to:</span></p>
                              <ul class="c12 lst-kix_list_2-0 start">
                                <li class="c6 c17 li-bullet-0"><span class="c1">&quot;What data or context do you need to do this better?&quot;:
                                    Prompting the AI to identify missing information or context that would improve its understanding or performance
                                    on a given task. This leverages the LLM&#39;s internal knowledge of what constitutes effective input.</span>
                                </li>
                                <li class="c8 li-bullet-0"><span class="c1">&quot;How should this be structured?&quot;: Asking the AI to suggest
                                    optimal structures, formats, or logical flows for a prompt or a multi-step workflow. This taps into the
                                    LLM&#39;s ability to recognize and generate patterns.</span></li>
                                <li class="c8 li-bullet-0"><span class="c1">&quot;Can you generate the optimal prompt for this outcome for tool or
                                    workflow?&quot;: Directing the AI to construct a complete, optimized prompt tailored for a specific desired
                                    outcome, potentially for use with other tools or within a larger automated workflow. This utilizes the AI&#39;s
                                    generative capabilities for prompt creation itself.</span></li>
                              </ul>
                              <p class="c14"><span class="c1">Metaprompting represents a powerful paradigm shift, enabling prompt engineers to
                                  leverage the LLM&#39;s intelligence not just for task execution, but for the meta-task of prompt design,
                                  accelerating the development of highly effective AI applications.</span></p>
                              <h2 class="c6"><span class="c19">IV. Module 3: Practical Application and Gemini-Specific Prompting</span></h2>
                              <h3 class="c6"><span class="c0">Leveraging Gemini&#39;s Long Context Window</span></h3>
                              <h4 class="c6"><span class="c0">Understanding Context Window Capabilities (1 Million Tokens)</span></h4>
                              <p class="c3"><span class="c1">Gemini distinguishes itself as the first generative model capable of processing an
                                  unprecedented 1 million tokens in a single context window. This represents a significant leap from earlier models,
                                  which were typically limited to 8,000, 32,000, or 128,000 tokens.[3] To provide a tangible scale, a 1 million
                                  token context window is equivalent to approximately 50,000 lines of code (assuming 80 characters per line), the
                                  entirety of text messages sent over five years, eight average-length English novels, or the transcripts of over
                                  200 average-length podcast episodes.[3]</span></p>
                              <p class="c3"><span class="c13">Gemini&#39;s 1-million-token context window fundamentally re-architects how
                                  information is managed and processed by LLMs, enabling a &quot;direct approach&quot; to complex tasks that
                                  previously required external retrieval or state management systems. The sheer capacity of this context window
                                  eliminates the necessity for complex external systems, such as Retrieval Augmented Generation (RAG) or sliding
                                  window techniques [3], for many long-form text tasks.[3] This simplification streamlines the development stack for
                                  certain applications. This implies that prompt engineers working with Gemini can adopt a more
                                  &quot;data-rich&quot; prompting strategy. Instead of meticulously curating minimal context, they can provide
                                  entire documents, extensive codebases, or complete conversation histories directly to the model. This shifts the
                                  engineering challenge from </span><span class="c15">retrieving</span><span class="c13">&nbsp;information to
                                </span><span class="c15">structuring and optimizing the presentation</span><span class="c1">&nbsp;of that
                                  information within the vast context window, for instance, by considering optimal query placement or utilizing
                                  context caching. This capability also makes &quot;many-shot in-context learning&quot; a viable and powerful
                                  alternative to traditional fine-tuning for certain applications.</span></p>
                              <h4 class="c6"><span class="c0">Best Practices for Query Placement and Information Density</span></h4>
                              <p class="c3"><span class="c1">For optimal model performance, particularly with long contexts, it is generally
                                  recommended to place the primary query or question at the end of the prompt, following all other contextual
                                  information.[3] While Gemini models are highly capable of extracting specific information from large volumes of
                                  tokens, demonstrating up to 99% accuracy in many cases, it is a general best practice to avoid passing tokens to
                                  the model if they are not explicitly needed for the task.[3]</span></p>
                              <h4 class="c6"><span class="c0">Optimizations: Context Caching for Cost and Latency</span></h4>
                              <p class="c3"><span class="c1">Context caching emerges as the primary optimization strategy when working with
                                  Gemini&#39;s long context window. This technique significantly reduces operational costs by allowing the reuse of
                                  a similar set of tokens or context across multiple requests without re-incurring the full input token cost for
                                  each interaction.[3] For example, Gemini Flash offers approximately 4x lower input/output cost compared to the
                                  standard model when utilizing context caching.[3] Beyond cost savings, context caching can also contribute to
                                  lower latency in certain scenarios, improving response times.[3]</span></p>
                              <h4 class="c6"><span class="c0">Use Cases: Summarization, Q&amp;A, Agentic Workflows, Many-Shot Learning</span></h4>
                              <p class="c6"><span class="c1">The extensive context window of Gemini opens up numerous advanced use cases:</span></p>
                              <ul class="c12 lst-kix_list_3-0 start">
                                <li class="c6 c17 li-bullet-0"><span class="c1">Summarizing large corpuses of text: This capability eliminates the
                                    need for complex techniques like sliding windows or maintaining state across different sections, which were
                                    necessary with models having smaller context limits.[3]</span></li>
                                <li class="c8 li-bullet-0"><span class="c1">Question and answering: Historically, sophisticated Q&amp;A systems
                                    often relied on Retrieval Augmented Generation (RAG) due to limited context and models&#39; lower factual
                                    recall. Gemini&#39;s long context allows for direct, in-context Q&amp;A over vast amounts of
                                    information.[3]</span></li>
                                <li class="c8 li-bullet-0"><span class="c1">Agentic workflows: The ability to provide sufficient information about
                                    the world and an agent&#39;s goals significantly improves the reliability of agentic workflows, as it is crucial
                                    for agents to maintain a comprehensive state of their operations and objectives.[3]</span></li>
                                <li class="c8 li-bullet-0"><span class="c1">Many-shot in-context learning: Scaling up the provision of examples from
                                    single or few-shot instances to hundreds, thousands, or even hundreds of thousands can unlock novel model
                                    capabilities and achieve performance levels comparable to fine-tuned models.[3]</span></li>
                              </ul>
                              <h3 class="c23"><span class="c0">Multimodal Prompting with Gemini</span></h3>
                              <p class="c3"><span class="c1">Gemini models are architected to be inherently multimodal from their foundation,
                                  offering native understanding of text, video, audio, and images.[3, 20] This design unlocks a broad spectrum of
                                  image processing and computer vision tasks, such as image captioning, classification, and visual question
                                  answering, without the need for training specialized machine learning models.[20] For larger media files, the File
                                  API supports uploads of up to 2GB per file, with a project limit of 20GB, enabling their seamless inclusion in
                                  prompts.[21]</span></p>
                              <p class="c3"><span class="c13">Gemini&#39;s native multimodal understanding and generation capabilities represent a
                                  significant advancement towards more natural and human-like AI interaction, enabling applications that seamlessly
                                  blend different forms of information. Unlike prior models that might process modalities separately or require
                                  complex integration, Gemini&#39;s inherent multimodality means it can &quot;reason&quot; across diverse data
                                  types. For example, it can answer questions based on an image (Visual Question Answering), analyze video content
                                  to provide recommendations [22], or generate images </span><span class="c15">within</span><span class="c1">&nbsp;a
                                  text response.[23] This is not merely parallel processing but integrated comprehension. This capability opens up
                                  entirely new categories of applications that were previously difficult or impossible to develop, such as
                                  interactive visual assistants (e.g., Project Astra[3]), dynamic content creation (e.g., interleaved text and
                                  images), and sophisticated media analysis tools. It expands the scope of prompt engineering beyond text-only
                                  inputs to encompass the design of prompts that effectively combine and leverage information from various
                                  modalities, closely mimicking how humans perceive and interact with the world.</span></p>
                              <h4 class="c6"><span class="c0">Image Understanding: Captioning, Classification, Visual Q&amp;A</span></h4>
                              <p class="c3"><span class="c1">Gemini models facilitate a range of image understanding tasks, including the generation
                                  of descriptive captions for images, image classification, and Visual Question Answering (VQA), where the model
                                  responds to questions posed about an input image.[20, 22] The models demonstrate robust multimodal comprehension
                                  capabilities across diverse visual content, including charts, natural photographs, and memes.[22]</span></p>
                              <h4 class="c6"><span class="c0">Video Understanding: Captioning, Q&amp;A, Memory, Real-time Processing</span></h4>
                              <p class="c3"><span class="c1">Gemini&#39;s capabilities extend to video understanding, encompassing tasks such as
                                  video question and answering, video memory (as exemplified by Google&#39;s Project Astra), generating video
                                  captions, and supporting real-time video processing applications.[3, 22]</span></p>
                              <h4 class="c6"><span class="c0">Audio Understanding: Transcription, Translation, Summarization</span></h4>
                              <p class="c3"><span class="c1">For audio data, Gemini supports real-time transcription and translation, question
                                  answering based on podcast or video audio, and summarization of meeting transcripts.[3]</span></p>
                              <h4 class="c6"><span class="c0">Interleaved Text-Image/Video Generation</span></h4>
                              <p class="c3"><span class="c1">Gemini 2.0 Flash possesses the capability to generate interleaved images alongside its
                                  text responses. This feature allows for the creation of rich, integrated content, such as illustrated recipes
                                  where images accompany each step of the text, or stories that include visuals alongside the narrative, all within
                                  a single model interaction.[23]</span></p>
                              <h3 class="c6"><span class="c0">Gemini-Specific Prompting Strategies &amp; Best Practices</span></h3>
                              <h4 class="c6"><span class="c0">Optimal Number of Examples for Gemini</span></h4>
                              <p class="c3"><span class="c1">While Gemini&#39;s large context window enables &quot;many-shot&quot; in-context
                                  learning, which can involve hundreds or thousands of examples, experimentation remains crucial to determine the
                                  optimal number for specific tasks. Providing an excessive number of examples can still lead to overfitting, where
                                  the model becomes too specialized to the provided examples and performs poorly on new, unseen data.[3, 5]</span>
                              </p>
                              <p class="c3"><span class="c13">While Gemini&#39;s extensive context window enables &quot;many-shot&quot; learning,
                                  the fundamental principles of optimal example quantity and consistent formatting remain critical. This indicates
                                  that raw input capacity alone does not negate the need for thoughtful prompt design. The &quot;many-shot&quot;
                                  capability refers to the model&#39;s inherent capacity to process large volumes of examples. Conversely, the
                                  concept of &quot;optimal number&quot; and the risk of &quot;overfitting&quot; pertain to the effectiveness of the
                                  prompt design for a specific task. Simply because Gemini </span><span class="c15">can</span><span
                                  class="c13">&nbsp;handle 1 million tokens does not mean that all 1 million tokens should be filled with examples
                                  for every task. The quality, diversity, and relevance of the examples provided still hold paramount importance.
                                  This implies that prompt engineers must carefully balance Gemini&#39;s immense capacity with sound pedagogical
                                  principles. They should strategically select examples, ensure variety, and maintain consistent formatting, even
                                  when providing a large number of them. The challenge shifts from </span><span class="c15">fitting</span><span
                                  class="c13">&nbsp;examples into a small window to </span><span class="c15">optimizing the pedagogical
                                  impact</span><span class="c1">&nbsp;of a potentially massive number of examples within Gemini&#39;s
                                  context.</span></p>
                              <h4 class="c6"><span class="c0">Patterns vs. Anti-patterns in Gemini Prompting</span></h4>
                              <p class="c3"><span class="c1">When providing examples, it is more effective to demonstrate positive patterns that the
                                  model should follow rather than illustrating anti-patterns that it should avoid. This guides the model towards
                                  desired behaviors more directly.[5]</span></p>
                              <h4 class="c6"><span class="c0">Consistent Formatting for Gemini Responses</span></h4>
                              <p class="c3"><span class="c1">Maintaining consistent structure and formatting within few-shot examples is essential.
                                  Inconsistencies can lead to undesired response formats from the model.[5]</span></p>
                              <h4 class="c6"><span class="c0">Handling Safety Filters and Fallback Responses</span></h4>
                              <p class="c3"><span class="c1">It is important to be aware that prompts or generated responses may trigger safety
                                  filters, which can result in fallback responses instead of the intended output. In some instances, increasing the
                                  temperature parameter might help to bypass these filters, though this should be approached with caution and
                                  ethical considerations.[5]</span></p>
                              <h3 class="c6"><span class="c0">Table 3: Hands-on Lab Exercises &amp; Objectives</span></h3>
                              <table class="c11"></table>
                              <p class="c16"><span class="c0"></span></p>
                              <table class="c11">
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Lab Number</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Lab Title</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Key Prompt Engineering Techniques Covered</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Gemini-Specific Features Utilized</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Learning Objectives</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Example Task/Scenario</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Lab 1</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Basic Text Generation and Formatting</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Zero-shot, Few-shot, Instruction-based, Parameter Tuning (Temperature, Max Output
                                        Tokens)</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Gemini&#39;s ability to follow explicit formatting instructions; general text
                                        generation capabilities.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Practice crafting clear instructions; understand impact of basic parameters;
                                        generate structured text (summaries, lists).</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Finance Scenario: As a junior analyst at &quot;CapitalFlow Investments,&quot;
                                        summarize a 500-word financial news article into a single, jargon-free sentence for a client email. Then,
                                        generate a bulleted list of 3 pros and 3 cons of investing in a specific emerging market, based on provided
                                        research notes.</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Lab 2</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Implementing Chain-of-Thought for Reasoning Tasks</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Chain-of-Thought (Zero-shot CoT, Few-shot CoT)</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Gemini&#39;s enhanced reasoning with CoT; processing multi-step problems.</span>
                                    </p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Apply CoT to solve arithmetic/logic problems; analyze step-by-step reasoning;
                                        compare CoT vs. direct prompting.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Finance Scenario: A client at &quot;WealthGuard Financial&quot; wants to
                                        calculate their projected retirement savings. Given their current savings, annual contribution, and
                                        estimated annual return, use CoT to break down and solve the multi-step calculation. Then, explain each step
                                        of the calculation to the client in simple terms.</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Lab 3</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Exploring Gemini&#39;s Long Context for Document Summarization</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Long Context utilization, Query Placement, Information Density</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Gemini&#39;s 1 million token context window; in-context learning for large
                                        documents.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Summarize extensive documents (e.g., multiple articles, book chapters); observe
                                        performance with varying query placements; understand context window advantages.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Finance Scenario: You are a compliance officer at &quot;SecureTrust Bank.&quot;
                                        Summarize a 70-page regulatory document (e.g., a new SEC filing) focusing on its core implications for
                                        consumer lending. Then, answer specific questions about clauses related to data privacy, which are located
                                        deep within the document.</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Lab 4</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Multimodal Prompting: Image/Video Analysis and Generation</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Multimodal Prompting (Image/Video/Audio input), Visual Q&amp;A, Interleaved
                                        Generation</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Gemini&#39;s native multimodal understanding (images, video, audio); File API for
                                        media uploads; interleaved text-image output.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Perform visual question answering; generate image captions; create interleaved
                                        text-image content (e.g., illustrated recipe); analyze video content for recommendations.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Finance Scenario: As a market researcher for &quot;Global Insight
                                        Analytics,&quot; analyze a provided image of a stock chart (e.g., showing a specific trend or pattern). Ask
                                        Gemini to identify the pattern and predict potential short-term movements. Then, generate a brief market
                                        commentary that includes an interleaved image of a similar chart pattern for illustration.</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Lab 5</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Building a Simple Agentic Workflow with Chained Prompts</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Iterative Refinement, Chained Prompts, Aggregation of Responses</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Gemini&#39;s capability for multi-turn interactions; breaking down complex
                                        tasks.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Design multi-step workflows; understand output as input; aggregate parallel
                                        results for final output.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Finance Scenario: Automate a client report generation process for
                                        &quot;PortfolioPro Advisors.&quot; Design a chained prompt workflow that first summarizes a company&#39;s
                                        latest earnings call transcript, then extracts key financial metrics (revenue, profit, EPS), and finally
                                        generates a concise, actionable investment recommendation based on these metrics.</span></p>
                                  </td>
                                </tr>
                                <tr class="c10">
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Project</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Developing a Gemini-powered Application</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">All learned techniques, System Design, Iterative Development</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Comprehensive application of Gemini&#39;s features.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Apply prompt engineering principles to a real-world problem; develop a functional
                                        mini-application; consolidate course knowledge.</span></p>
                                  </td>
                                  <td class="c5" colspan="1" rowspan="1">
                                    <p class="c4"><span class="c2">Finance Scenario: Develop a &quot;Personal Budget Assistant&quot;
                                        mini-application for &quot;FinWise Solutions.&quot; This app should take user income and expense categories,
                                        use Gemini to categorize unstructured spending descriptions, and then generate a weekly budget summary with
                                        personalized saving tips.</span></p>
                                  </td>
                                </tr>
                              </table>
                              <h2 class="c18"><span class="c19">V. Conclusion and Future Directions</span></h2>
                              <h3 class="c6"><span class="c0">Summary of Key Learnings</span></h3>
                              <p class="c3"><span class="c1">This comprehensive course outline underscores the pivotal role of prompt engineering as
                                  a non-parametric methodology for significantly extending the capabilities of Large Language Models. It has
                                  highlighted Gemini&#39;s transformative features, particularly its expansive 1 million token context window and
                                  native multimodal understanding, and how these capabilities fundamentally reshape existing prompting strategies.
                                  The curriculum progresses systematically, illustrating the evolution from foundational techniques like zero-shot
                                  and few-shot prompting to advanced paradigms such as Chain-of-Thought, Tree-of-Thought, and Graph of Thoughts. A
                                  consistent theme throughout is the iterative and adaptive nature of effective prompt design, emphasizing
                                  continuous refinement.</span></p>
                              <h3 class="c6"><span class="c0">Emerging Trends in Prompt Engineering</span></h3>
                              <p class="c3"><span class="c1">The field of prompt engineering is characterized by dynamic and rapid advancements.
                                  Future directions include continued research into the interpretability of prompts, seeking to understand precisely
                                  why certain linguistic cues are effective, as exemplified by metrics like the ZIP score.[9] There is a growing
                                  focus on the development of more sophisticated, graph-based reasoning frameworks, such as Graph of Thoughts, which
                                  aim to enable LLMs to emulate complex, non-linear human thought processes.[19] The automation of prompt
                                  optimization and iterative refinement processes, as demonstrated by approaches like Prochemy, is also a
                                  significant trend, seeking to streamline and enhance the efficiency of prompt design.[13] Furthermore, the
                                  integration of LLMs into complex agentic systems and multi-turn workflows represents a paradigm shift from
                                  single-query interactions to designing intelligent, autonomous AI systems capable of sequential and collaborative
                                  tasks.[3, 5] Addressing the inherent challenges in long-context prompting, particularly mitigating issues like
                                  positional bias observed in techniques such as Self-Consistency, remains a critical area of ongoing research and
                                  development.[16]</span></p>
                              <h3 class="c6"><span class="c0">Challenges and Ethical Considerations</span></h3>
                              <p class="c3"><span class="c1">Despite the significant advancements, several challenges and ethical considerations
                                  persist within prompt engineering.</span></p>
                              <h4 class="c6"><span class="c0">Limitations of Current Techniques</span></h4>
                              <p class="c3"><span class="c1">Certain advanced techniques, such as Self-Consistency, demonstrate limitations,
                                  particularly in long-context scenarios where their performance can degrade due to issues like positional bias.[16]
                                  This indicates that simply scaling up context does not universally guarantee improved results and necessitates the
                                  development of more robust, context-aware methodologies.</span></p>
                              <h4 class="c6"><span class="c0">Bias and Toxicity</span></h4>
                              <p class="c3"><span class="c1">A critical ethical concern is the potential for prompt engineering, especially through
                                  techniques like persona prompting, to inadvertently amplify harmful content generation and introduce or exacerbate
                                  biases within LLM outputs.[11] This risk necessitates careful design and rigorous evaluation to prevent unintended
                                  negative societal impacts.</span></p>
                              <h4 class="c6"><span class="c0">Security Implications</span></h4>
                              <p class="c3"><span class="c1">While not extensively detailed in the provided materials, the broader field of prompt
                                  engineering inherently faces security implications, including vulnerabilities to prompt injection attacks and
                                  other adversarial manipulations. These threats can compromise model integrity and lead to undesirable or malicious
                                  outputs.</span></p>
                              <h4 class="c6"><span class="c0">Responsible AI Development</span></h4>
                              <p class="c3"><span class="c1">Given these challenges, a paramount consideration is the commitment to responsible AI
                                  development. This involves emphasizing ethical prompt design principles and implementing continuous evaluation
                                  frameworks to identify and mitigate unintended consequences. The ongoing evolution of prompt engineering demands a
                                  proactive approach to ensure that these powerful models are deployed safely, fairly, and effectively for the
                                  benefit of society.</span></p>
                            </body>
                            
                            </html>
                    </div>
                </div>
            </section>
        </main>
    </div>

    <div id="modal" class="fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center p-4 z-50 hidden">
        <div id="modal-content" class="bg-white rounded-lg shadow-xl max-w-4xl w-full max-h-[90vh] overflow-y-auto p-6 relative">
            <button onclick="closeModal()" class="absolute top-4 right-4 text-gray-500 hover:text-gray-800 text-2xl" aria-label="Close modal">&times;</button>
            <h2 id="modal-title" class="text-2xl font-bold mb-4"></h2>
            <div id="modal-body" class="text-gray-700 space-y-4"></div>
        </div>
    </div>

    <script type="module">
        import { initializeApp } from 'https://www.gstatic.com/firebasejs/11.10.0/firebase-app.js';
        import { getAuth, onAuthStateChanged, signInWithEmailAndPassword, createUserWithEmailAndPassword, signOut, GoogleAuthProvider, signInWithPopup } from 'https://www.gstatic.com/firebasejs/11.10.0/firebase-auth.js';

        // Your Firebase project configuration
        const firebaseConfig = {
            apiKey: "AIzaSyCHea_F73VgiBUZazpPwbcrd6f3OJuwjN0",
            authDomain: "prompt-engg-learning-app.firebaseapp.com",
            projectId: "prompt-engg-learning-app",
            storageBucket: "prompt-engg-learning-app.appspot.com",
            messagingSenderId: "402291199689",
            appId: "1:402291199689:web:9ee70af2ba568d8c778718"
        };

        // Initialize Firebase
        const app = initializeApp(firebaseConfig);
        const auth = getAuth(app);
        const googleProvider = new GoogleAuthProvider(); // Initialize Google Auth Provider

        // Declare contextChartInstance globally
        let contextChartInstance;

        // Data for the techniques section (MOVED TO TOP)
        const techniquesData = [
            { 
                title: "Zero-shot Prompting", 
                desc: "Direct task instruction with no examples provided. The model relies solely on its pre-trained knowledge to understand and respond to the prompt. This method has demonstrably improved the performance of LLMs across various applications.", 
                useCase: "Ideal for simple, general tasks where the model has a high probability of understanding the intent without prior examples, such as direct questions or basic summaries. The surprising effectiveness of certain zero-shot prompts, such as the simple phrase 'Let's think step-by-step,' points to the presence of emergent reasoning capabilities within large models.", 
                mistake: "Being too vague or general, leading to irrelevant or unhelpful outputs. For example, just saying 'Describe this' without further context. This indicates that prompt engineering for zero-shot tasks is less about providing explicit examples and more about identifying the optimal meta-instructions that align the model's internal cognitive processes with the desired task.", 
                gemini: "Gemini handles clean, explicit zero-shot instructions exceptionally well. For complex zero-shot tasks, adding cues like 'Let's think step-by-step' can activate its reasoning abilities effectively. This direction encourages further exploration into 'prompt interpretability,' as exemplified by metrics like the ZIP score, to understand precisely why certain words or phrases exert a more substantial influence on model output, leading to more principled zero-shot prompt design.",
                examplePrompt: "Write a short, engaging product description for a new smart thermostat that learns user preferences.",
                expectedOutput: "Introducing the EcoSense Smart Thermostat: Your home's intuitive climate control. This intelligent device learns your preferences, optimizes energy use, and adapts to your schedule, saving you money and enhancing comfort effortlessly. Experience smart living, simplified."
            },
            { 
                title: "Few-shot Prompting", 
                desc: "Involves supplying models with a small collection of input-output examples to help them comprehend a given task. Even a limited number of high-quality examples can substantially improve model performance, particularly on complex tasks.", 
                useCase: "Frequently employed to regulate the formatting, phrasing, scope, or general patterning of generated responses. Effective for tasks requiring a specific tone, style, few-shot classification, complex reasoning with examples, or adherence to a strict output format. It's a powerful in-context learning method.", 
                mistake: "Using inconsistent or overly complex examples that confuse the model. Providing too many examples can also lead to overfitting or consume too much context window without proportional gain. The effectiveness is not solely dependent on *what* examples are provided, but also on *how many*, *how diverse*, and *how consistently they are formatted*.", 
                gemini: "Gemini benefits significantly from well-crafted few-shot examples. Its large context window allows for more examples if needed, but quality and consistency of examples are more critical than quantity to avoid overfitting. This often requires iterative testing and potentially automated strategies for example selection to maximize the benefits while mitigating potential drawbacks.",
                examplePrompt: `Translate the following sentences into French:\n\nEnglish: Hello -> French: Bonjour\nEnglish: Thank you -> French: Merci\nEnglish: Goodbye -> French:`,
                expectedOutput: `French: Au revoir`
            },
            { 
                title: "Instruction-based Prompting", 
                desc: "Involves the meticulous design of directives that explicitly guide the model's behavior and output. The clarity and precision of these instructions are paramount for ensuring that the model performs tasks as intended, thereby avoiding ambiguity and misinterpretations.", 
                useCase: "Ensuring precise task execution, avoiding ambiguity, controlling specificity, and defining boundaries for output. Examples include 'Summarize in 3 bullet points,' or 'Respond in JSON format.' Fundamentally, prompts function as natural language instructions that provide the necessary context for the model to operate.", 
                mistake: "Overly general instructions lacking specificity or desired constraints, or contradictory instructions. This can lead to the model making unwanted assumptions or generating off-topic content. The model functions as a highly capable instruction-follower but cannot infer unstated intent.", 
                gemini: "Gemini responds exceptionally well to clear, precise, and well-structured directives. This technique enables fine-grained control over its output, making it highly effective for controlled generation. This often entails breaking down complex tasks into simpler, more direct directives, employing explicit constraints, and clearly defining desired output formats, effectively 'programming' the LLM through natural language.",
                examplePrompt: `Summarize the following text in exactly 50 words:\n\n"The quick brown fox jumps over the lazy dog. This sentence is often used to test typewriters and computer keyboards because it contains all letters of the English alphabet. It's a pangram, a sentence that contains every letter of the alphabet at least once. Its simplicity and completeness make it a popular choice for demonstrations and tests in typography and computing."`,
                expectedOutput: `The quick brown fox jumps over the lazy dog is a pangram, a sentence containing every English alphabet letter. Used for testing typewriters and keyboards, its simplicity and completeness make it popular for typography and computing demonstrations.`
            },
            { 
                title: "Role-based Prompting (Persona)", 
                desc: "Assigns a specific identity, persona, context, or behavioral framework to the model, influencing its tone, style, and approach to the given task. This technique has been shown to enhance reasoning capabilities, particularly when the chosen persona is domain-specific.", 
                useCase: "Tasks requiring a specific tone (e.g., 'Act as a professional journalist'), domain expertise simulation (e.g., 'You are a senior data analyst'), or emulating a particular character's perspective.", 
                mistake: "Not adequately specifying how the role should influence behavior, or assigning roles that introduce unintended biases or limitations. The persona must be relevant and well-defined. This technique is akin to a 'double-edged sword,' as it also carries the significant risk of increasing the generation of harmful content and amplifying toxicity.", 
                gemini: "Role clarity helps Gemini adopt the desired tone and content effectively. However, it requires careful consideration to avoid increasing potential for toxicity or unintended bias based on the assigned persona. This mandates that any application employing persona prompting must integrate strong safety filters and adhere to stringent ethical considerations.",
                examplePrompt: `Act as a seasoned travel blogger. Write a vivid paragraph describing a hidden gem beach in Southeast Asia.`,
                expectedOutput: `Nestled away from the bustling tourist trails, Maya Bay on Koh Phi Phi Leh in Thailand remains a true hidden gem. Its pristine turquoise waters, framed by towering limestone cliffs and powdery white sand, create an otherworldly paradise. The vibrant coral reefs just offshore teem with marine life, making it a snorkeler's dream. Accessible only by longtail boat through a narrow channel, the journey itself adds to the allure, revealing a secluded haven untouched by mass tourism.`
            }
        ];

        // Data for the advanced techniques section
        const advancedTechniquesData = [
            {
                title: "Chain-of-Thought (CoT)",
                desc: "Guides the model to perform step-by-step reasoning, improving accuracy on complex arithmetic, commonsense, and symbolic reasoning tasks. It simulates human-like thinking processes.",
                examplePrompt: "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? Let's think step by step.",
                expectedOutput: "Roger started with 5 tennis balls.\nHe bought 2 more cans, and each can has 3 tennis balls. So, he bought 2 * 3 = 6 tennis balls.\nNow, he has 5 + 6 = 11 tennis balls."
            },
            {
                title: "Tree-of-Thought (ToT)",
                desc: "Extends CoT by exploring multiple reasoning paths. It allows for backtracking and self-correction, much like a search tree, to arrive at more optimal solutions for difficult problems.",
                examplePrompt: "Problem: Design a marketing campaign for a new eco-friendly water bottle. Consider target audience, key message, and channels. Explore at least two distinct approaches and evaluate their pros and cons. Think step by step, considering multiple paths.",
                expectedOutput: "Thought 1: Define target audience. (e.g., environmentally conscious millennials, outdoor enthusiasts)\nThought 2: Brainstorm key messages for each audience.\nThought 3: Explore channels for each message.\nPath A: Focus on social media influencers...\nPath B: Focus on partnerships with outdoor gear brands...\nEvaluate Path A vs. Path B..."
            },
            {
                title: "Graph-of-Thoughts (GoT)",
                desc: "Further generalizes ToT, allowing for arbitrary graph structures of thoughts. This enables highly flexible and synergistic reasoning where different thought nodes can influence each other to achieve superior outcomes.",
                examplePrompt: "Task: Generate a comprehensive business plan for a sustainable urban farm. Break down the problem into interconnected modules: Market Analysis, Operations, Financial Projections, and Team. Allow for cross-module feedback and refinement. Provide a final integrated plan.",
                expectedOutput: "Node 1: Market Analysis (Initial Draft)\nNode 2: Operations Plan (Initial Draft)\nNode 3: Financial Projections (Initial Draft)\nNode 4: Team Structure (Initial Draft)\nFeedback Loop 1: Market Analysis informs Operations.\nFeedback Loop 2: Operations impact Financials.\nRefinement of Node 1 based on Node 2.\nIntegration of all refined nodes into Final Business Plan."
            },
            {
                title: "Self-Consistency (SC)",
                desc: "A robust strategy designed to mitigate failures in LLM outputs by aggregating multiple sampled responses. This technique operates on the premise that diverse reasoning paths, generated through stochastic sampling (e.g., by using a non-zero temperature), will converge towards correct answers when their final outputs are aggregated, often via a majority voting mechanism.",
                examplePrompt: "Question: If a train travels at 60 mph for 2 hours, and then at 40 mph for 1 hour, what is the total distance traveled? (Generate 3 distinct answers and then pick the most consistent one).",
                expectedOutput: "Response 1: (Calculates 120 + 40 = 160 miles)\nResponse 2: (Calculates 120 + 40 = 160 miles)\nResponse 3: (Calculates 120 + 40 = 160 miles)\nConsistent Answer: 160 miles"
            },
            {
                title: "Iterative Refinement & Chaining",
                desc: "This approach treats prompting like a software development lifecycle. Complex tasks are broken down into a chain of simpler, sequential prompts. Each prompt's output serves as the input for the next, and the entire chain can be refined systematically based on feedback, leading to more robust and sophisticated AI workflows.",
                examplePrompt: `Prompt 1 (Summarization): "Summarize the following article: [Article Text]"\nPrompt 2 (Extraction, using output of P1): "From the summary above, extract all named entities (people, organizations, locations)."\nPrompt 3 (Question Generation, using output of P2): "Generate 3 insightful questions based on the extracted entities: [Entities List]"`,
                expectedOutput: `Example Chained Prompts:\nPrompt 1 (Summarization): "Summarize the following article: [Article Text]"\nPrompt 2 (Extraction, using output of P1): "From the summary above, extract all named entities (people, organizations, locations)."\nPrompt 3 (Question Generation, using output of P2): "Generate 3 insightful questions based on the extracted entities: [Entities List]"\n\nExample Iterative Refinement:\nInitial Prompt: "Write a short story about a detective."\nFeedback Prompt: "The previous story was too generic. Make the detective a cynical ex-cop in a futuristic city, and add a plot twist involving a rogue AI."`
            }
        ];

        // Data for the labs section (MOVED TO TOP)
        const labsData = [
            { 
                num: 1, 
                title: "Basic Text Generation and Formatting", 
                techniques: "Zero-shot, Few-shot, Instruction-based, Parameter Tuning (Temperature, Max Output Tokens)", 
                features: "Gemini's ability to follow explicit formatting instructions; general text generation capabilities.", 
                objective: "To practice crafting clear and concise instructions for basic text generation, understand the impact of fundamental inference parameters (Temperature, Top-K, Top-P), and generate structured text outputs.", 
                task: "As a junior analyst at \"CapitalFlow Investments,\" summarize a 500-word financial news article into a single, jargon-free sentence for a client email. Then, generate a bulleted list of 3 pros and 3 cons of investing in a specific emerging market, based on provided research notes.",
                examplePrompt: `Summarize the following financial article into one concise, jargon-free sentence for a client email:\n\n"The latest quarterly report from TechInnovate Corp. shows a surprising surge in revenue driven by their new AI division, exceeding analyst expectations by 15%. However, increased operational costs due to supply chain disruptions have slightly impacted net profit margins. The company's stock reacted positively, jumping 8% in early trading, but market sentiment remains cautious regarding long-term sustainability given geopolitical tensions."\n\nThen, generate a bulleted list of 3 pros and 3 cons of investing in the 'Quantum Computing' emerging market, based on the following research notes:\n\nResearch Notes:\n- Pros: Potential for exponential growth, revolutionary impact on industries (finance, healthcare), government funding for research, early-mover advantage for select companies.\n- Cons: High R&D costs, long development cycles, significant technological hurdles, limited current applications, high market volatility, intense competition.`,
                expectedOutput: `TechInnovate Corp. reported strong revenue growth from AI, but higher costs impacted profits, leading to cautious stock gains.\n\nPros of investing in Quantum Computing:\n- Potential for exponential growth\n- Revolutionary impact on industries\n- Government funding for research\n\nCons of investing in Quantum Computing:\n- High R&D costs\n- Long development cycles\n- Significant technological hurdles`
            },
            { 
                num: 2, 
                title: "Implementing Chain-of-Thought for Reasoning Tasks", 
                techniques: "Chain-of-Thought (Zero-shot CoT, Few-shot CoT)", 
                features: "Gemini's enhanced reasoning with CoT; processing multi-step problems.", 
                objective: "To apply Chain-of-Thought prompting to solve complex arithmetic/logic problems, and to analyze the detailed step-by-step thought processes generated by the model. Compare CoT vs. direct prompting.", 
                task: "A client at \"WealthGuard Financial\" wants to calculate their projected retirement savings. Given their current savings, annual contribution, and estimated annual return, use CoT to break down and solve the multi-step calculation. Then, explain each step of the calculation to the client in simple terms.",
                examplePrompt: `You are a financial advisor. A client has $100,000 in current savings. They plan to contribute $5,000 annually for the next 10 years. The estimated annual return on investment is 7%. Using a Chain-of-Thought approach, calculate their projected retirement savings after 10 years. Assume annual compounding. Then, explain each step of the calculation in simple terms for the client.\n\nLet's think step by step.`,
                expectedOutput: `Step 1: Calculate the future value of the initial savings.\n   Future Value = Present Value * (1 + Rate)^Number of Periods\n   FV_initial = $100,000 * (1 + 0.07)^10 = $100,000 * 1.96715 = $196,715\n\nStep 2: Calculate the future value of the annual contributions (annuity).\n   Future Value of Annuity = Pmt * [((1 + r)^n - 1) / r]\n   FV_annuity = $5,000 * [((1 + 0.07)^10 - 1) / 0.07]\n   FV_annuity = $5,000 * [0.96715 / 0.07]\n   FV_annuity = $5,000 * 13.8164 = $69,082\n\nStep 3: Add the future values from Step 1 and Step 2 to get the total projected savings.\n   Total Projected Savings = $196,715 + $69,082 = $265,797\n\nExplanation for the client:\n1. Your initial savings of $100,000 will grow to about $196,715 over 10 years, earning 7% interest annually.\n2. Your yearly contributions of $5,000 will accumulate to approximately $69,082 over the same 10 years, also earning 7% annually.\n3. Combining these, your total projected retirement savings after 10 years will be around $265,797.`
            },
            {
                num: 3, 
                title: "Exploring Gemini's Long Context for Document Summarization", 
                techniques: "Long Context utilization, Query Placement, Information Density", 
                features: "Gemini's 1 million token context window; in-context learning for large documents.", 
                objective: "To effectively utilize Gemini's extended context window for processing and summarizing extensive documents, and to observe the impact of query placement on information retrieval accuracy within long texts.", 
                task: "You are a compliance officer at \"SecureTrust Bank.\" Summarize a 70-page regulatory document (e.g., a new SEC filing) focusing on its core implications for consumer lending. Then, answer specific questions about clauses related to data privacy, which are located deep within the document.",
                examplePrompt: `You are a compliance officer at SecureTrust Bank. Your task is to analyze a new 70-page regulatory document (imagine this is a very long document provided here: [EXTENSIVE REGULATORY DOCUMENT TEXT]).\n\nFirst, summarize the core implications of this document specifically for consumer lending practices at SecureTrust Bank. Focus on new requirements or significant changes.\n\nSecond, answer the following questions based on the full document, paying close attention to details that might be buried deep within the text:\n1. What are the new requirements regarding customer data consent for third-party sharing?\n2. Are there any specific clauses addressing the retention period for loan application data?\n3. What are the penalties for non-compliance related to data breaches, as outlined in this document?\n\nPlace your answers to the questions after the summary.`,
                expectedOutput: `[Generated Summary of Core Implications for Consumer Lending]\n\n1. New requirements for customer data consent for third-party sharing include [specific details from document].\n2. The document specifies that loan application data must be retained for [specific retention period] under clause [clause number/section].\n3. Penalties for non-compliance related to data breaches, as outlined, include [specific penalties and conditions].`
            },
            { 
                num: 4, 
                title: "Multimodal Prompting: Image/Video Analysis and Generation", 
                techniques: "Multimodal Prompting (Image/Video/Audio input), Visual Q&amp;A, Interleaved Generation", 
                features: "Gemini's native multimodal understanding (images, video, audio); File API for media uploads; interleaved text-image output.", 
                objective: "Perform visual question answering; generate image captions; create interleaved text-image content (e.g., illustrated recipe); analyze video content for recommendations.", 
                task: "As a market researcher for \"Global Insight Analytics,\" analyze a provided image of a stock chart (e.g., showing a specific trend or pattern). Ask Gemini to identify the pattern and predict potential short-term movements. Then, generate a brief market commentary that includes an interleaved image of a similar chart pattern for illustration.",
                examplePrompt: `You are a market researcher for Global Insight Analytics. Analyze the provided image of a stock chart: .\n\n1. Identify the primary chart pattern visible (e.g., Head and Shoulders, Double Top, Ascending Triangle, etc.).\n2. Based on this pattern, predict the potential short-term price movement (e.g., bullish, bearish, consolidation) and explain your reasoning.\n3. Generate a brief market commentary (max 100 words) discussing this pattern's implications, and include a placeholder for an interleaved image that visually represents a similar pattern for educational purposes.\n\nOutput Format:\nPattern Identified: [Pattern Name]\nPredicted Movement: [Prediction]\nReasoning: [Explanation]\nMarket Commentary: [Text with IMAGE_PLACEHOLDER for interleaved image]`,
                expectedOutput: `Pattern Identified: Ascending Triangle\nPredicted Movement: Bullish breakout\nReasoning: The chart shows higher lows and a flat resistance level, indicating increasing buying pressure and a likely upward breakout.\nMarket Commentary: The stock is exhibiting a classic ascending triangle pattern, a bullish signal often preceding an upward price move. Traders are closely watching the resistance level for a potential breakout, suggesting continued upward momentum. IMAGE_PLACEHOLDER This pattern often leads to significant gains.`
            },
            { 
                num: 5, 
                title: "Building a Simple Agentic Workflow with Chained Prompts", 
                techniques: "Iterative Refinement, Chained Prompts, Aggregation of Responses", 
                features: "Gemini's capability for multi-turn interactions; breaking down complex tasks.", 
                objective: "Design multi-step workflows; understand output as input; aggregate parallel results for final output.", 
                task: "Automate a client report generation process for \"PortfolioPro Advisors.\" Design a chained prompt workflow that first summarizes a company's latest earnings call transcript, then extracts key financial metrics (revenue, profit, EPS), and finally generates a concise, actionable investment recommendation based on these metrics.",
                examplePrompt: `You are an AI assistant for PortfolioPro Advisors. Your task is to generate a concise investment recommendation for a client based on a company's earnings call transcript.\n\n**Workflow:**\n1. **Summarize Earnings Call:** Summarize the provided earnings call transcript into 3-5 key bullet points, focusing on financial performance and future outlook.\n   Transcript: [EARNINGS CALL TRANSCRIPT TEXT]\n\n2. **Extract Key Metrics:** From the summary generated in Step 1, extract the following key financial metrics: Revenue, Net Profit, and Earnings Per Share (EPS). Present them as a JSON object.\n\n3. **Generate Recommendation:** Based on the extracted metrics from Step 2, provide a concise, actionable investment recommendation (Buy, Sell, or Hold) for a conservative investor, along with a brief justification (max 50 words).\n\n---`,
                expectedOutput: `**Step 1: Summarize Earnings Call**\n- Revenue increased by X% year-over-year, driven by [factor].\n- Net profit grew by Y%, exceeding expectations.\n- Company projects Z% growth for the next quarter due to [initiatives].\n- Supply chain challenges are easing, but inflation remains a concern.\n\n**Step 2: Extract Key Metrics**\n\`\`\`json\n{\n  "Revenue": "[Value]",\n  "Net Profit": "[Value]",
  "EPS": "[Value]"\n}\n\`\`\`\n\n**Step 3: Generate Recommendation**\nRecommendation: [Buy/Sell/Hold]\nJustification: [Brief justification based on metrics and conservative investor profile, max 50 words].`
            },
            {
                num: 6,
                title: "Developing a Gemini-powered Application",
                techniques: "All learned techniques, System Design, Iterative Development",
                features: "Comprehensive application of Gemini's features.",
                objective: "Apply prompt engineering principles to a real-world problem; develop a functional mini-application; consolidate course knowledge.",
                task: "Develop a \"Personal Budget Assistant\" mini-application for \"FinWise Solutions.\" This app should take user income and expense categories, use Gemini to categorize unstructured spending descriptions, and then generate a weekly budget summary with personalized saving tips.",
                examplePrompt: `You are the core AI for a "Personal Budget Assistant" at FinWise Solutions. Your goal is to help users understand and manage their spending.\n\n**Input:**\nUser Income: $5000/month\nExpense Categories: Housing, Food, Transportation, Entertainment, Utilities, Savings, Miscellaneous\nUnstructured Spending Descriptions (comma-separated): "Coffee shop $5", "Rent payment $1500", "Groceries $120", "Movie tickets $30", "Bus fare $2.50", "Electricity bill $80", "Dinner with friends $45", "Savings transfer $500"\n\n**Task:**\n1. **Categorize Spending:** For each unstructured spending description, categorize it into one of the provided Expense Categories. If a category doesn't fit well, use "Miscellaneous".\n2. **Calculate Category Totals:** Sum the spending for each category.\n3. **Generate Weekly Budget Summary:** Provide a summary of spending per category, remaining budget for each, and overall financial health. Highlight any categories where spending is high relative to income.\n4. **Personalized Saving Tip:** Based on the spending patterns, generate one actionable, personalized saving tip for the user (max 30 words).\n\n**Output Format:**\n**Categorized Spending:**\n[List each item with its category]\n\n**Weekly Budget Summary:**\n[Summary of spending per category, remaining budget, overall financial health]\n\n**Personalized Saving Tip:**\n[One actionable tip]`,
                expectedOutput: `**Categorized Spending:**\n- Coffee shop $5: Food\n- Rent payment $1500: Housing\n- Groceries $120: Food\n- Movie tickets $30: Entertainment\n- Bus fare $2.50: Transportation\n- Electricity bill $80: Utilities\n- Dinner with friends $45: Food\n- Savings transfer $500: Savings\n\n**Weekly Budget Summary:**\nFood: $170 (Remaining: $...)\nHousing: $1500 (Remaining: $...)\nTransportation: $2.50 (Remaining: $...)\nEntertainment: $30 (Remaining: $...)\nUtilities: $80 (Remaining: $...)\nSavings: $500 (Remaining: $...)\nMiscellaneous: $0 (Remaining: $...)\nOverall Financial Health: [Assessment based on spending vs. income]\n\n**Personalized Saving Tip:**\nConsider packing lunch more often to reduce daily food expenses and boost your savings.`
            }
        ];


        // Listen for authentication state changes
        onAuthStateChanged(auth, (user) => {
            if (user) {
                // User is signed in. Display the app content.
                document.getElementById('app-content').classList.remove('hidden');
                document.getElementById('login-screen').classList.add('hidden');
            } else {
                // User is signed out. Show the login screen.
                document.getElementById('app-content').classList.add('hidden');
                document.getElementById('login-screen').classList.remove('hidden');
            }
        });

        // Function to handle login with Google
        window.loginWithGoogle = async () => {
            const errorMessageElement = document.getElementById('login-error-message');
            errorMessageElement.innerText = ''; // Clear previous errors

            try {
                await signInWithPopup(auth, googleProvider);
                // User will be redirected by onAuthStateChanged
            } catch (error) {
                console.error("Google login failed:", error.message);
                errorMessageElement.innerText = "Google login failed: " + error.message;
            }
        };

        // Function to handle logout (optional, but good practice)
        window.logoutUser = async () => {
            try {
                await signOut(auth);
            } catch (error) {
                console.error("Logout failed:", error.message);
            }
        };

        // Make showSection globally accessible
        window.showSection = function(sectionId) {
            document.querySelectorAll('.content-section').forEach(section => {
                section.classList.remove('active');
            });
            document.getElementById(sectionId).classList.add('active');

            document.querySelectorAll('.nav-link').forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('onclick').includes(sectionId)) {
                    link.classList.add('active');
                }
            });
             if (sectionId === 'practice') {
                renderContextChart();
            }
        }

        // Make openModal globally accessible
        window.openModal = function(techTitle) { 
            const tech = techniquesData.find(t => t.title === techTitle); 
            if (!tech) return; 

            document.getElementById('modal-title').innerText = tech.title;
            document.getElementById('modal-body').innerHTML = `
                <p><strong>Description:</strong> ${tech.desc}</p>
                <p><strong>When to Use:</strong> ${tech.useCase}</p>
                <p><strong>Common Mistake:</strong> ${tech.mistake}</p>
                <div class="p-4 bg-blue-50 border-l-4 border-blue-500 mt-4">
                    <h4 class="font-semibold text-blue-800">Gemini-Specific Considerations</h4>
                    <p class="text-blue-700">${tech.gemini}</p>
                </div>
                <div class="mt-4 p-3 bg-gray-100 rounded-md">
                    <p class="font-semibold">Example Prompt:</p>
                    <pre class="whitespace-pre-wrap text-sm">${tech.examplePrompt.replace(/`/g, '&#96;')}</pre>
                    <p class="font-semibold mt-2">Expected Output:</p>
                    <pre class="whitespace-pre-wrap text-sm">${tech.expectedOutput.replace(/`/g, '&#96;')}</pre>
                </div>
                <button id="generate-example-btn" class="mt-6 px-4 py-2 bg-blue-500 text-white rounded-md hover:bg-blue-600 transition-colors">Generate Example Prompt ‚ú®</button>
                <div id="llm-example-loading" class="loading-text hidden"></div>
                <div id="llm-example-output" class="llm-output hidden"></div>
            `;
            // Increased modal width for better content fitting
            document.getElementById('modal-content').classList.remove('max-w-2xl');
            document.getElementById('modal-content').classList.add('max-w-4xl');
            document.getElementById('modal').classList.remove('hidden');

            document.getElementById('generate-example-btn').addEventListener('click', function() {
                window.generateExamplePrompt(tech.title, this); // Call globally accessible function
            });
        }

        // Make closeModal globally accessible
        window.closeModal = function() {
            document.getElementById('modal').classList.add('hidden');
        }

        // Make toggleLab globally accessible
        window.toggleLab = function(element) {
            const labItem = element.closest('.lab-item');
            labItem.classList.toggle('open');
            if (!labItem.classList.contains('open')) {
                const outputDiv = labItem.querySelector('.llm-output');
                if (outputDiv) outputDiv.innerHTML = '';
                const loadingText = labItem.querySelector('.loading-text');
                if (loadingText) loadingText.innerHTML = ''; // Clear loading text on close
            }
        }

        // Make setupParameterExplorer globally accessible
        window.setupParameterExplorer = function() {
            const tempSlider = document.getElementById('temperature');
            const topkSlider = document.getElementById('top-k');
            const toppSlider = document.getElementById('top-p');

            tempSlider.addEventListener('input', (e) => {
                document.getElementById('temp-value').innerText = e.target.value;
            });
            topkSlider.addEventListener('input', (e) => {
                document.getElementById('topk-value').innerText = e.target.value;
            });
            toppSlider.addEventListener('input', (e) => {
                document.getElementById('topp-value').innerText = e.target.value;
            });
        }

        // Make renderTechniques globally accessible
        window.renderTechniques = function() {
            const container = document.getElementById('techniques-container');
            container.innerHTML = techniquesData.map(tech => `
                <div class="card p-6 cursor-pointer" onclick="openModal('${tech.title}')"> 
                    <h3 class="text-xl font-bold mb-2">${tech.title}</h3>
                    <p class="text-gray-600">${tech.desc.split('.')[0]}.</p> 
                </div>
            `).join('');
        }

        // New function to render advanced techniques
        window.renderAdvancedTechniques = function() {
            const container = document.getElementById('advanced-techniques-container');
            // Prepare the cards and arrows
            const verticalCards = ["Chain-of-Thought (CoT)", "Tree-of-Thought (ToT)", "Graph-of-Thoughts (GoT)"];
            let cardsHtml = '';
            verticalCards.forEach((title, idx) => {
                const tech = advancedTechniquesData.find(t => t.title === title);
                if (!tech) return;
                cardsHtml += `
                    <div class="p-4 rounded-lg bg-blue-50 border border-blue-200 w-full max-w-4xl mx-auto">
                        <h4 class="font-bold">${tech.title}</h4>
                        <p class="text-sm text-gray-600">${tech.desc}</p>
                        <div class="mt-4 p-3 bg-gray-100 rounded-md text-left">
                            <p class="font-semibold">Example Prompt:</p>
                            <pre class="whitespace-pre-wrap text-sm">${tech.examplePrompt.replace(/`/g, '&#96;')}</pre>
                            <p class="font-semibold mt-2">Expected Output (partial):</p>
                            <pre class="whitespace-pre-wrap text-sm">${tech.expectedOutput.replace(/`/g, '&#96;')}</pre>
                        </div>
                        <button class="mt-4 px-4 py-2 bg-blue-500 text-white rounded-md hover:bg-blue-600 transition-colors" onclick="window.generateAdvancedExamplePrompt('${tech.title}', this)">Generate Example Prompt ‚ú®</button>
                        <div class="loading-text hidden"></div>
                        <div class="llm-output hidden"></div>
                    </div>
                `;
                // Add arrow except after the last card
                if (idx < verticalCards.length - 1) {
                    cardsHtml += `<div class='text-4xl text-blue-400 my-2' aria-hidden='true'>&#8595;</div>`;
                }
            });
            container.innerHTML = `
                <div class="card p-6">
                    <h3 class="text-xl font-bold text-blue-600 mb-3">The Evolution of Reasoning: From Linear to Graph Structures</h3>
                    <p class="text-gray-600 mb-6">Advanced prompting has moved beyond simple linear thoughts to explore complex, interconnected reasoning paths, enabling more robust problem-solving.</p>
                    <div class="flex flex-col items-center justify-center gap-6 text-center">
                        ${cardsHtml}
                    </div>
                </div>
                <div class="grid md:grid-cols-2 gap-8">
                    ${advancedTechniquesData.filter(t => ["Self-Consistency (SC)", "Iterative Refinement & Chaining"].includes(t.title)).map(tech => `
                        <div class="card p-6">
                            <h3 class="text-xl font-bold mb-2">${tech.title}</h3>
                            <p class="text-gray-600 mb-4">${tech.desc}</p>
                            ${tech.title === "Self-Consistency (SC)" ? `
                            <div class="p-3 bg-red-50 border border-red-200 rounded-lg">
                                <h4 class="font-semibold text-red-800">Long Context Limitation for SC</h4>
                                <p class="text-sm text-red-700">While powerful, SC's performance can degrade in very long contexts (e.g., >10k tokens) due to potential positional bias within the models. In such cases, all sampled responses tend to inherit the same structural biases related to token position, which fundamentally violates SC's core assumption of error independence. Consequently, instead of mitigating errors, sampling from a model with inherent positional bias can amplify them.</p>
                            </div>
                            ` : `
                            <ul class="list-disc list-inside text-gray-600 text-sm mt-3 space-y-1">
                                <li><strong>Chaining:</strong> Connecting multiple prompts where the output of one feeds into the next.</li>
                                <li><strong>Iterative Refinement:</strong> Continuously improving prompts and model outputs based on evaluation and feedback loops.</li>
                                <li><strong>Aggregating Parallel Responses:</strong> Performing different parallel sub-tasks on distinct portions of data and then aggregating the individual results to produce a comprehensive final output.</li>
                            </ul>
                            `}
                            <div class="mt-4 p-3 bg-gray-100 rounded-md text-left">
                                <p class="font-semibold">Example Prompt:</p>
                                <pre class="whitespace-pre-wrap text-sm">${tech.examplePrompt.replace(/`/g, '&#96;')}</pre>
                                <p class="font-semibold mt-2">Expected Output (conceptual):</p>
                                <pre class="whitespace-pre-wrap text-sm">${tech.expectedOutput.replace(/`/g, '&#96;')}</pre>
                            </div>
                            <button class="mt-4 px-4 py-2 bg-blue-500 text-white rounded-md hover:bg-blue-600 transition-colors" onclick="window.generateAdvancedExamplePrompt('${tech.title}', this)">Generate Example Prompt ‚ú®</button>
                            <div class="loading-text hidden"></div>
                            <div class="llm-output hidden"></div>
                        </div>
                    `).join('')}
                </div>
            `;
        }
        
        // Make renderLabs globally accessible
        window.renderLabs = function() {
            const container = document.getElementById('labs-container');
            container.innerHTML = labsData.map(lab => `
                 <div class="lab-item bg-white rounded-lg shadow-sm border border-gray-200">
                    <div class="p-4 cursor-pointer flex justify-between items-center" onclick="toggleLab(this)">
                        <h3 class="text-lg font-semibold"><span class="text-blue-500 mr-2">${lab.num}.</span>${lab.title}</h3>
                        <span class="text-2xl text-gray-400 transform transition-transform duration-300">&darr;</span>
                    </div>
                    <div class="lab-details px-4 pb-4">
                        <div class="border-t pt-4 text-gray-600 space-y-2">
                           <p><strong>Techniques:</strong> ${lab.techniques}</p>
                           <p><strong>Gemini Features:</strong> ${lab.features}</p>
                           <p><strong>Objective:</strong> ${lab.objective}</p>
                           <p><strong>Example Task:</strong> ${lab.task}</p>
                           <div class="mt-4 p-3 bg-gray-100 rounded-md">
                               <p class="font-semibold">Example Prompt:</p>
                               <pre class="whitespace-pre-wrap text-sm">${lab.examplePrompt.replace(/`/g, '&#96;')}</pre>
                               <p class="font-semibold mt-2">Expected Output:</p>
                               <pre class="whitespace-pre-wrap text-sm">${lab.expectedOutput.replace(/`/g, '&#96;')}</pre>
                           </div>
                        </div>
                    </div>
                </div>
            `).join('');
        }

        // Make renderContextChart globally accessible
        window.renderContextChart = function() {
            if (contextChartInstance) {
                contextChartInstance.destroy(); 
            }
            const ctx = document.getElementById('contextChart').getContext('2d');
            contextChartInstance = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['GPT-3.5 (4k)', 'GPT-4 (8k)', 'GPT-4 Turbo (128k)', 'Gemini 1.5 Pro (1M)'],
                    datasets: [{
                        label: 'Max Context Window (Tokens)',
                        data: [4096, 8192, 128000, 1000000],
                        backgroundColor: [
                            'rgba(255, 159, 64, 0.5)',
                            'rgba(75, 192, 192, 0.5)',
                            'rgba(153, 102, 255, 0.5)',
                            'rgba(54, 162, 235, 0.5)'
                        ],
                        borderColor: [
                            'rgb(255, 159, 64)',
                            'rgb(75, 192, 192)',
                            'rgb(153, 102, 255)',
                            'rgb(54, 162, 235)'
                        ],
                        borderWidth: 1
                    }]
                },
                options: {
                    indexAxis: 'y',
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: {
                            type: 'logarithmic',
                             ticks: {
                                callback: function(value, index, values) {
                                    if (value === 1000) return "1k";
                                    if (value === 10000) return "10k";
                                    if (value === 100000) return "100k";
                                    if (value === 1000000) return "1M";
                                    return null;
                                }
                            }
                        }
                    },
                    plugins: {
                        legend: {
                            display: false
                        },
                        title: {
                            display: true,
                            text: 'LLM Context Window Comparison (Logarithmic Scale)'
                        }
                    }
                }
            });
        }

        // Function to toggle mobile menu visibility
        window.toggleMobileMenu = function() {
            const mobileMenu = document.getElementById('mobile-menu');
            mobileMenu.classList.toggle('hidden');
        };

        // Make generateExamplePrompt and generateLabHint globally accessible
        window.generateExamplePrompt = async function(techTitle, buttonElement) {
            const outputDiv = document.getElementById('llm-example-output');
            const loadingText = document.getElementById('llm-example-loading');
            buttonElement.disabled = true;
            outputDiv.innerHTML = ''; // Clear previous output
            outputDiv.classList.add('hidden'); // Hide output until new content is ready
            loadingText.innerText = 'Generating example...'; // Set loading text
            loadingText.classList.remove('hidden'); // Show loading text

            try {
                let prompt = `Give me a concise example of a "${techTitle}" prompt for a large language model, along with a brief expected output for that prompt. Format it clearly as 'Prompt: [Your Prompt]\\nExpected Output: [Expected Output]'.`;
                
                let chatHistory = [];
                chatHistory.push({ role: "user", parts: [{ text: prompt }] });
                const payload = { contents: chatHistory };
                const apiKey = "AIzaSyB_uKPnCdFs5tlXYA7UJW2JDw8phL2k1Pk"; // Canvas will provide this at runtime
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });
                const result = await response.json();

                if (result.candidates && result.candidates.length > 0 &&
                    result.candidates[0].content && result.candidates[0].content.parts &&
                    result.candidates[0].content.parts.length > 0) {
                    const text = result.candidates[0].content.parts[0].text;
                    outputDiv.innerHTML = `<pre>${text}</pre>`;
                    outputDiv.classList.remove('hidden'); // Ensure output is visible
                } else {
                    outputDiv.innerText = "Could not generate example. Please try again.";
                    outputDiv.classList.remove('hidden');
                }
            } catch (error) {
                console.error("Error generating example:", error);
                outputDiv.innerText = "Error generating example: " + error.message;
                outputDiv.classList.remove('hidden');
            } finally {
                loadingText.classList.add('hidden');
                buttonElement.disabled = false;
            }
        }

        // New function to generate example prompts for advanced techniques
        window.generateAdvancedExamplePrompt = async function(techTitle, buttonElement) {
            const parentDiv = buttonElement.parentElement;
            const loadingText = parentDiv.querySelector('.loading-text');
            const outputDiv = parentDiv.querySelector('.llm-output');

            buttonElement.disabled = true;
            outputDiv.innerHTML = ''; // Clear previous output
            outputDiv.classList.add('hidden'); // Hide output until new content is ready
            loadingText.innerText = 'Generating example...'; // Set loading text
            loadingText.classList.remove('hidden'); // Show loading text

            try {
                let prompt = `Give me a concise example of a "${techTitle}" prompt for a large language model, along with a brief expected output for that prompt. Format it clearly as 'Prompt: [Your Prompt]\\nExpected Output: [Expected Output]'.`;
                
                let chatHistory = [];
                chatHistory.push({ role: "user", parts: [{ text: prompt }] });
                const payload = { contents: chatHistory };
                const apiKey = "AIzaSyB_uKPnCdFs5tlXYA7UJW2JDw8phL2k1Pk"; // Canvas will provide this at runtime
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });
                const result = await response.json();

                if (result.candidates && result.candidates.length > 0 &&
                    result.candidates[0].content && result.candidates[0].content.parts &&
                    result.candidates[0].content.parts.length > 0) {
                    const text = result.candidates[0].content.parts[0].text;
                    outputDiv.innerHTML = `<pre>${text}</pre>`;
                    outputDiv.classList.remove('hidden'); // Ensure output is visible
                } else {
                    outputDiv.innerText = "Could not generate example. Please try again.";
                    outputDiv.classList.remove('hidden');
                }
            } catch (error) {
                console.error("Error generating example:", error);
                outputDiv.innerText = "Error generating example: " + error.message;
                outputDiv.classList.remove('hidden');
            } finally {
                loadingText.classList.add('hidden');
                buttonElement.disabled = false;
            }
        }

        // Email/password login
        window.loginWithEmailPassword = async () => {
            const email = document.getElementById('login-email').value;
            const password = document.getElementById('login-password').value;
            const errorMessageElement = document.getElementById('login-error-message');
            errorMessageElement.innerText = '';
            try {
                await signInWithEmailAndPassword(auth, email, password);
            } catch (error) {
                errorMessageElement.innerText = "Login failed: " + error.message;
            }
        };

        document.addEventListener('DOMContentLoaded', () => {
            renderTechniques();
            setupParameterExplorer();
            renderAdvancedTechniques(); // Call the new function
            renderLabs();
            // Attach modal click listener here as well
            document.getElementById('modal').addEventListener('click', (e) => {
                if (e.target.id === 'modal') {
                    closeModal();
                }
            });

            // Event listener for mobile menu button
            const mobileMenuButton = document.getElementById('mobile-menu-button');
            if (mobileMenuButton) {
                mobileMenuButton.addEventListener('click', window.toggleMobileMenu);
            }
        });
    </script>
</body>
</html>